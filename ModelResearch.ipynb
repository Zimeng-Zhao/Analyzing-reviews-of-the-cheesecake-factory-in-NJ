{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import json\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "with open(\"data/labeled_sentense.json\", \"rb\") as file:\n",
    "    datalist = json.load(file)\n",
    "\n",
    "with open(\"data/labeled_sentense2.json\", \"rb\") as file:\n",
    "    datalist+=json.load(file)\n",
    "\n",
    "with open(\"data/labeled_sentense3.json\",\"rb\") as file:\n",
    "    datalist+=json.load(file)\n",
    "\n",
    "with open(\"data/labeled_sentense4.json\",\"rb\") as file:\n",
    "    datalist+=json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data num: 20000\n",
      "food: 7860\n",
      "atmosphere: 1351\n",
      "parking: 47\n",
      "service: 4967\n",
      "none: 3386\n",
      "hygiene: 498\n",
      "location: 397\n",
      "None: 280\n",
      "food, service: 102\n",
      "Food: 294\n",
      "Service: 459\n",
      "Atmosphere: 118\n",
      "Hygiene: 60\n",
      "food, service, atmosphere: 10\n",
      "None.: 1\n",
      "food, atmosphere: 23\n",
      "transportation: 22\n",
      "food, hygiene, atmosphere, service, location: 1\n",
      "Location: 25\n",
      "hygiene, food, service: 1\n",
      "food\n",
      ": 1\n",
      "food and service: 9\n",
      "Error: 500, {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that!\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      ": 6\n",
      "food and atmosphere: 2\n",
      "food and transportation: 1\n",
      "Label: location: 1\n",
      "Great! Let's get started. Please provide the comment for me to label.: 1\n",
      "service and food: 1\n",
      "service, hygiene: 1\n",
      "food, hygiene: 5\n",
      "Parking: 4\n",
      "food, hygiene, atmosphere, service, location, parking, transportation, none: 1\n",
      "Transportation: 2\n",
      "food + service: 1\n",
      "food,service: 1\n",
      "atmosphere, service: 4\n",
      "Food, hygiene: 1\n",
      "Food.: 1\n",
      "Great! Let's get started then. Please provide the first comment for labeling.: 1\n",
      "busy.: 1\n",
      "Restaurant name: none: 1\n",
      "atmosphere, food, service: 1\n",
      "food, atmosphere, service: 11\n",
      "Sure, please provide me with a comment and I will give it a label.: 1\n",
      "Food, service: 1\n",
      "I apologize, but your comment is incomplete. Please provide a complete comment and I will be able to assist you in labeling its topic.: 1\n",
      "atmosphere, service, food: 1\n",
      "service, food: 2\n",
      "location, atmosphere: 1\n",
      "The label for this comment is \"hygiene\".: 1\n",
      "food/location: 1\n",
      "comfort: 1\n",
      "comment is given, I will ask you to label it with one of the eight topic categories mentioned earlier. So let's begin! Please provide the comment and I will help you with the labeling.: 1\n",
      "service\n",
      "none: 1\n",
      "Error: 503, {\"error\":{\"code\":503,\"message\":\"Service Unavailable.\",\"param\":null,\"type\":\"cf_service_unavailable\"}}: 2\n",
      "would label this comment as \"food\" since it mentions grease on the fingers.: 1\n",
      "waiting time: 2\n",
      "food, location: 2\n",
      "food+service: 1\n",
      "service and atmosphere: 1\n",
      "Comment: \"The food here is delicious and the portions are generous.\" \n",
      "Label: food: 1\n",
      "out. None: 1\n",
      "Great! Let's begin with the first comment:\n",
      "\n",
      "Comment 1: \"The food at this restaurant is absolutely delicious!\"\n",
      "\n",
      "Label: food: 1\n",
      " food: 1\n",
      "Price: 1\n",
      "Atmosphere.: 1\n",
      "food: subpar \n",
      "service: quick \n",
      "staff: non-responsive: 1\n",
      "Yes, the unique choices are food, hygiene, atmosphere, service, location, parking, transportation, and none.: 1\n",
      "Hygiene, location, parking: 1\n",
      "seating location: 1\n",
      "SERVICE: 1\n",
      "price: 1\n",
      "Food, Service: 1\n",
      "Sorry, could you please provide more context or clarify your question?: 1\n",
      "all-food: 1\n",
      "Great! Let's get started. Please provide me with the first comment about a restaurant and I will assign a suitable label to it.: 1\n",
      "food, hygiene, atmosphere: 1\n",
      "service\n",
      "hygiene\n",
      "food\n",
      "food: 1\n",
      "Great! Let's get started. Please provide me with the first comment.: 1\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for data in datalist:\n",
    "    sentense=data[0]\n",
    "    label=data[1]\n",
    "    X.append(sentense)\n",
    "    y.append(label)\n",
    "print(\"Data num: \"+str(len(X)))\n",
    "#print(set(y))\n",
    "\n",
    "mydict={}\n",
    "\n",
    "for d in y:\n",
    "    mydict.setdefault(d,0)\n",
    "    mydict[d]=mydict[d]+1\n",
    "\n",
    "for key in mydict:\n",
    "    print(key+\": \"+str(mydict[key]))   \n",
    "\n",
    "#print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data num 19776\n",
      "food: 8157\n",
      "atmosphere: 1470\n",
      "location: 497\n",
      "service: 5427\n",
      "none: 3667\n",
      "hygiene: 558\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "X=[]\n",
    "y=[]\n",
    "for data in datalist:\n",
    "    sentense=data[0]\n",
    "    label=re.sub(r'^[^a-zA-Z]+|[^a-zA-Z]+$', '', data[1]).lower()\n",
    "    if label==\"none\" or label==\"food\" or label==\"service\" or label==\"atmosphere\" or label==\"hygiene\":\n",
    "        X.append(sentense)\n",
    "        y.append(label)\n",
    "    elif label==\"location\" or label==\"parking\" or label==\"transportation\":\n",
    "        X.append(sentense)\n",
    "        y.append(\"location\")\n",
    "print(\"Data num\",len(X))\n",
    "\n",
    "mydict={}\n",
    "\n",
    "for d in y:\n",
    "    mydict.setdefault(d,0)\n",
    "    mydict[d]=mydict[d]+1\n",
    "\n",
    "for key in mydict:\n",
    "    print(key+\": \"+str(mydict[key]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def balanceData(X,y,mininum,maxnum):\n",
    "    mydict={}\n",
    "    for i in range(len(y)):\n",
    "        mydict.setdefault(y[i],[])\n",
    "        mydict[y[i]].append(X[i])\n",
    "    for key in mydict:\n",
    "        originalnum=len(mydict[key])\n",
    "        if originalnum<mininum:\n",
    "            for i in range(mininum-originalnum):\n",
    "                index=random.randint(0,len(mydict[key])-1)\n",
    "                X.append(mydict[key][index])\n",
    "                y.append(key)\n",
    "        else:\n",
    "            for i in range(originalnum-maxnum):\n",
    "                index=random.randint(0,len(mydict[key])-1)\n",
    "                deleted=mydict[key][index]\n",
    "                index2=X.index(deleted)\n",
    "                del mydict[key][index]\n",
    "                del X[index2]\n",
    "                del y[index2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanceData(X,y,800,700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food': 8157, 'atmosphere': 1470, 'location': 497, 'service': 5427, 'none': 3667, 'hygiene': 558}\n"
     ]
    }
   ],
   "source": [
    "mydict={}\n",
    "\n",
    "for d in y:\n",
    "    mydict.setdefault(d,0)\n",
    "    mydict[d]=mydict[d]+1\n",
    "\n",
    "    \n",
    "\n",
    "print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "categories = list(mydict.keys())\n",
    "values = list(mydict.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(categories, values, color='skyblue')\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), va='bottom', ha='center')\n",
    "\n",
    "plt.title('Customer Feedback Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Feedbacks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer,cosine_distance,euclidean_distance\n",
    "from sklearn import mixture\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def cluster_kmean(train_text, test_text, train_label,test_label):\n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.0001,lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text)\n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 8000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 4000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    train_dtm,_ = sm.fit_resample(train_dtm, train_label)\n",
    "    test_dtm=tfidf.transform(test_text)\n",
    "    num_clusters = 30\n",
    "    clusterer = KMeansClusterer(num_clusters, \\\n",
    "                                cosine_distance, \\\n",
    "                                repeats=1)\n",
    "    clusterer.cluster(train_dtm.toarray(), assign_clusters=True)\n",
    "    predicted = [clusterer.classify(v) for v in test_dtm.toarray()]\n",
    "\n",
    "    confusion_df = pd.DataFrame(list(zip(test_label, predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "    cross=pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)\n",
    "    labelmap = dict(zip(cross.index,cross.idxmax(axis=1)))\n",
    "    predicted_target=[labelmap[i] \\\n",
    "                  for i in predicted]\n",
    "    print(metrics.classification_report(test_label, predicted_target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\cluster\\util.py:130: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  atmosphere       0.35      0.21      0.26       277\n",
      "        food       0.68      0.74      0.71      1641\n",
      "     hygiene       0.51      0.32      0.39       107\n",
      "    location       0.58      0.37      0.45        94\n",
      "        none       0.44      0.37      0.40       763\n",
      "     service       0.58      0.67      0.62      1074\n",
      "\n",
      "    accuracy                           0.59      3956\n",
      "   macro avg       0.52      0.45      0.47      3956\n",
      "weighted avg       0.58      0.59      0.58      3956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_kmean(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_gmm(train_text, test_text, train_label, test_label):\n",
    "    \n",
    " \n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,max_features=1000,lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text)\n",
    "    test_dtm=tfidf.transform(test_text)\n",
    "    #sampling_strategy = {'food': 10000, 'atmosphere': 10000, 'location': 8000, 'service': 10000, 'none': 10000, 'hygiene': 10000}\n",
    "    #sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    #train_dtm,_ = sm.fit_resample(train_dtm, train_label)\n",
    "\n",
    "    lowest_bic = np.infty   # initial BIC is set to infinity\n",
    "    best_gmm = None\n",
    "\n",
    "    n_components_range = range(5,20)    # The number of clusters\n",
    "\n",
    "    cv_types = ['spherical', 'tied', 'diag']  \n",
    "    for cv_type in cv_types:\n",
    "    \n",
    "        for n_components in n_components_range:\n",
    "            \n",
    "            # Fit a Gaussian mixture with EM\n",
    "            gmm = mixture.GaussianMixture(n_components=n_components,\n",
    "                                        # n_init=2\n",
    "                                        covariance_type=cv_type, random_state=42,n_init=10)\n",
    "            gmm.fit(train_dtm.toarray())\n",
    "            \n",
    "            bic = gmm.bic(train_dtm.toarray())  # get Model BIC\n",
    "            \n",
    "            if bic < lowest_bic:  # save the model with lowest BIC sofar\n",
    "                lowest_bic = bic\n",
    "                best_gmm = gmm\n",
    "    predicted = best_gmm.predict(test_dtm.toarray())\n",
    "    confusion_df = pd.DataFrame(list(zip(test_label, predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "    #print(confusion_df.head())\n",
    "    cross=pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)\n",
    "    #print(cross)\n",
    "    labelmap = dict(zip(cross.index,cross.idxmax(axis=1)))\n",
    "    #print(labelmap)\n",
    "    predicted_target=[labelmap[i] \\\n",
    "                  for i in predicted]\n",
    "    print(metrics.classification_report(test_label, predicted_target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  atmosphere       0.00      0.00      0.00       277\n",
      "        food       0.57      0.65      0.61      1641\n",
      "     hygiene       0.00      0.00      0.00       107\n",
      "    location       0.00      0.00      0.00        94\n",
      "        none       0.35      0.50      0.41       763\n",
      "     service       0.51      0.46      0.49      1074\n",
      "\n",
      "    accuracy                           0.49      3956\n",
      "   macro avg       0.24      0.27      0.25      3956\n",
      "weighted avg       0.44      0.49      0.46      3956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cluster_gmm(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "def cluster_lda(train_text, test_text, train_label, test_label):\n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,max_features=3000,ngram_range=(1,1),lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text)\n",
    "    test_dtm=tfidf.transform(test_text)\n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 6000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 8000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    train_dtm,_ = sm.fit_resample(train_dtm, train_label)\n",
    "    num_clusters = 30\n",
    "    lda = LatentDirichletAllocation(n_components=num_clusters, \\\n",
    "                                    max_iter=40,verbose=0,\n",
    "                                    evaluate_every=1, n_jobs=1,\n",
    "                                    random_state=0).fit(train_dtm)\n",
    "    topic_distributions = lda.transform(test_dtm)\n",
    "    #print(topic_distributions)\n",
    "    # Assign the most prominent topic to each sentence\n",
    "    predicted = topic_distributions.argmax(axis=1)\n",
    "\n",
    "    #result=aggregateTopic(allsentense[:200],most_prominent_topic[:200])\n",
    "    confusion_df = pd.DataFrame(list(zip(test_label, predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "    #print(confusion_df.head())\n",
    "    cross=pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)\n",
    "    #print(cross)\n",
    "    labelmap = dict(zip(cross.index,cross.idxmax(axis=1)))\n",
    "    #print(labelmap)\n",
    "    predicted_target=[labelmap[i] \\\n",
    "                  for i in predicted]\n",
    "    print(metrics.classification_report(test_label, predicted_target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  atmosphere       0.00      0.00      0.00       277\n",
      "        food       0.53      0.75      0.62      1641\n",
      "     hygiene       0.00      0.00      0.00       107\n",
      "    location       0.00      0.00      0.00        94\n",
      "        none       0.39      0.39      0.39       763\n",
      "     service       0.48      0.39      0.43      1074\n",
      "\n",
      "    accuracy                           0.49      3956\n",
      "   macro avg       0.23      0.25      0.24      3956\n",
      "weighted avg       0.42      0.49      0.45      3956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cluster_lda(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMapping(topic_distributions,labels):\n",
    "    labelmap={}\n",
    "    for j in range(topic_distributions.shape[1]):\n",
    "        score={}\n",
    "        for i in range(topic_distributions.shape[0]):\n",
    "            score[labels[i]]=score.setdefault(labels[i],0)+topic_distributions[i][j]\n",
    "        maxkey=labels[0]\n",
    "        for key in score:\n",
    "            if score[key]>score[maxkey]:\n",
    "                maxkey=key\n",
    "        labelmap[j]=maxkey\n",
    "    return labelmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "def cluster_lda(train_text, test_text, train_label, test_label):\n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,max_features=3000,ngram_range=(1,1),lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text)\n",
    "    test_dtm=tfidf.transform(test_text)\n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 6000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 4000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    train_dtm,train_label = sm.fit_resample(train_dtm, train_label)\n",
    "    num_clusters = 30\n",
    "    lda = LatentDirichletAllocation(n_components=num_clusters, \\\n",
    "                                    max_iter=40,verbose=0,\n",
    "                                    evaluate_every=1, n_jobs=1,\n",
    "                                    random_state=0).fit(train_dtm)\n",
    "    train_topic_distributions = lda.transform(train_dtm)\n",
    "    test_topic_distributions = lda.transform(test_dtm)\n",
    "    #print(topic_distributions)\n",
    "    # Assign the most prominent topic to each sentence\n",
    "    predicted = test_topic_distributions.argmax(axis=1)\n",
    "\n",
    "    labelmap =getMapping(train_topic_distributions,train_label)\n",
    "    #print(labelmap)\n",
    "    predicted_target=[labelmap[i] \\\n",
    "                  for i in predicted]\n",
    "    print(metrics.classification_report(test_label, predicted_target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  atmosphere       0.16      0.06      0.09       277\n",
      "        food       0.56      0.59      0.58      1641\n",
      "     hygiene       0.13      0.33      0.19       107\n",
      "    location       0.24      0.22      0.23        94\n",
      "        none       0.28      0.17      0.21       763\n",
      "     service       0.37      0.45      0.41      1074\n",
      "\n",
      "    accuracy                           0.42      3956\n",
      "   macro avg       0.29      0.30      0.28      3956\n",
      "weighted avg       0.41      0.42      0.41      3956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_lda(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "def cluster_lda(train_text, test_text, train_label, test_label):\n",
    "    \n",
    "    lb = LabelBinarizer()            \n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,max_features=3000,ngram_range=(1,1),lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text)\n",
    "    test_dtm=tfidf.transform(test_text)\n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 6000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 8000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    #train_dtm,train_label = sm.fit_resample(train_dtm, train_label)\n",
    "    num_clusters = 10\n",
    "    lda = LatentDirichletAllocation(n_components=num_clusters, \\\n",
    "                                    max_iter=40,verbose=0,\n",
    "                                    evaluate_every=1, n_jobs=1,\n",
    "                                    random_state=0).fit(train_dtm)\n",
    "    \n",
    "\n",
    "    train_topic_distributions = lda.transform(train_dtm)\n",
    "    test_topic_distributions = lda.transform(test_dtm)\n",
    "\n",
    "\n",
    "    print(train_topic_distributions)\n",
    "    train_y=lb.fit_transform(train_label)\n",
    "    test_y=lb.transform(test_label)\n",
    "\n",
    "    clf=OneVsRestClassifier(LinearSVC())\n",
    "    clf.fit(train_topic_distributions,train_y)\n",
    "    y_pred=clf.predict(test_topic_distributions)\n",
    "    print(y_pred.shape)\n",
    "    report=classification_report(test_y,y_pred,target_names=lb.classes_)\n",
    "    print(report)\n",
    "    print(\"Accuracy:\", accuracy_score(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04147964 0.31375665 0.04148711 ... 0.04147964 0.04147964 0.35437633]\n",
      " [0.02758572 0.09801057 0.17750565 ... 0.02761039 0.02758572 0.09102718]\n",
      " [0.04208838 0.0420793  0.0420793  ... 0.04208386 0.04208422 0.26337682]\n",
      " ...\n",
      " [0.26242148 0.03663742 0.03663742 ... 0.03663742 0.03663742 0.03663742]\n",
      " [0.27420193 0.28175249 0.03705943 ... 0.03705943 0.03705943 0.18462957]\n",
      " [0.05       0.05       0.05       ... 0.05       0.05       0.05      ]]\n",
      "(3956, 6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  atmosphere       0.00      0.00      0.00       277\n",
      "        food       0.61      0.34      0.43      1641\n",
      "     hygiene       0.00      0.00      0.00       107\n",
      "    location       0.00      0.00      0.00        94\n",
      "        none       0.00      0.00      0.00       763\n",
      "     service       0.57      0.12      0.20      1074\n",
      "\n",
      "   micro avg       0.60      0.17      0.27      3956\n",
      "   macro avg       0.20      0.08      0.11      3956\n",
      "weighted avg       0.41      0.17      0.23      3956\n",
      " samples avg       0.17      0.17      0.17      3956\n",
      "\n",
      "Accuracy: 0.17214357937310415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "cluster_lda(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def cluster_lda(train_text, test_text, train_label, test_label):\n",
    "    \n",
    "    lb = LabelBinarizer()            \n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,ngram_range=(1,1),lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text)\n",
    "    test_dtm=tfidf.transform(test_text)\n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 6000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 8000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    #train_dtm,train_label = sm.fit_resample(train_dtm, train_label)\n",
    "    num_clusters = 128\n",
    "    lda = LatentDirichletAllocation(n_components=num_clusters, \\\n",
    "                                    max_iter=40,verbose=0,\n",
    "                                    evaluate_every=1, n_jobs=1,\n",
    "                                    random_state=0).fit(train_dtm)\n",
    "    \n",
    "\n",
    "    train_topic_distributions = lda.transform(train_dtm)\n",
    "    test_topic_distributions = lda.transform(test_dtm)\n",
    "\n",
    "\n",
    "    print(train_topic_distributions)\n",
    "    train_y=lb.fit_transform(train_label)\n",
    "    test_y=lb.transform(test_label)\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(train_topic_distributions.shape[1],)),\n",
    "    Dense(32, activation='sigmoid'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(6, activation='softmax')  \n",
    "])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled=scaler.fit_transform(train_topic_distributions)\n",
    "    X_test_scaled=scaler.fit_transform(test_topic_distributions)\n",
    "\n",
    "    model.fit(X_train_scaled, train_y, epochs=300, batch_size=64)\n",
    "\n",
    "    y_pred=model.predict(X_test_scaled)\n",
    "\n",
    "    y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_pred_binary = np.zeros_like(y_pred)\n",
    "    for i, class_index in enumerate(y_pred_argmax):\n",
    "        y_pred_binary[i, class_index] = 1\n",
    "\n",
    "\n",
    "\n",
    "    print(y_pred_binary.shape)\n",
    "    report=classification_report(test_y,y_pred_binary,target_names=lb.classes_)\n",
    "    print(report)\n",
    "    print(\"Accuracy:\", accuracy_score(test_y, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0032406  0.0032406  0.0032406  ... 0.0032406  0.0032406  0.0032406 ]\n",
      " [0.00215513 0.00215513 0.08796688 ... 0.00215513 0.00215513 0.00215513]\n",
      " [0.00328745 0.00328745 0.00328745 ... 0.00328745 0.00328745 0.00328745]\n",
      " ...\n",
      " [0.0028623  0.0028623  0.0028623  ... 0.0028623  0.0028623  0.21327116]\n",
      " [0.00289527 0.00289527 0.00289527 ... 0.00289527 0.00289527 0.00289527]\n",
      " [0.00390625 0.00390625 0.00390625 ... 0.00390625 0.00390625 0.00390625]]\n",
      "WARNING:tensorflow:From c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\HengZhao\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "248/248 [==============================] - 1s 930us/step - loss: 1.4064 - accuracy: 0.4347\n",
      "Epoch 2/300\n",
      "248/248 [==============================] - 0s 821us/step - loss: 1.1631 - accuracy: 0.5811\n",
      "Epoch 3/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 1.1024 - accuracy: 0.6058\n",
      "Epoch 4/300\n",
      "248/248 [==============================] - 0s 843us/step - loss: 1.0747 - accuracy: 0.6119\n",
      "Epoch 5/300\n",
      "248/248 [==============================] - 0s 829us/step - loss: 1.0563 - accuracy: 0.6171\n",
      "Epoch 6/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 1.0428 - accuracy: 0.6183\n",
      "Epoch 7/300\n",
      "248/248 [==============================] - 0s 840us/step - loss: 1.0295 - accuracy: 0.6180\n",
      "Epoch 8/300\n",
      "248/248 [==============================] - 0s 851us/step - loss: 1.0169 - accuracy: 0.6224\n",
      "Epoch 9/300\n",
      "248/248 [==============================] - 0s 841us/step - loss: 1.0056 - accuracy: 0.6266\n",
      "Epoch 10/300\n",
      "248/248 [==============================] - 0s 841us/step - loss: 0.9970 - accuracy: 0.6281\n",
      "Epoch 11/300\n",
      "248/248 [==============================] - 0s 846us/step - loss: 0.9914 - accuracy: 0.6288\n",
      "Epoch 12/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.9865 - accuracy: 0.6291\n",
      "Epoch 13/300\n",
      "248/248 [==============================] - 0s 843us/step - loss: 0.9828 - accuracy: 0.6319\n",
      "Epoch 14/300\n",
      "248/248 [==============================] - 0s 809us/step - loss: 0.9774 - accuracy: 0.6336\n",
      "Epoch 15/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.9707 - accuracy: 0.6376\n",
      "Epoch 16/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.9667 - accuracy: 0.6380\n",
      "Epoch 17/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.9623 - accuracy: 0.6402\n",
      "Epoch 18/300\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.9560 - accuracy: 0.6440\n",
      "Epoch 19/300\n",
      "248/248 [==============================] - 0s 831us/step - loss: 0.9517 - accuracy: 0.6422\n",
      "Epoch 20/300\n",
      "248/248 [==============================] - 0s 841us/step - loss: 0.9472 - accuracy: 0.6434\n",
      "Epoch 21/300\n",
      "248/248 [==============================] - 0s 833us/step - loss: 0.9455 - accuracy: 0.6437\n",
      "Epoch 22/300\n",
      "248/248 [==============================] - 0s 837us/step - loss: 0.9415 - accuracy: 0.6451\n",
      "Epoch 23/300\n",
      "248/248 [==============================] - 0s 835us/step - loss: 0.9410 - accuracy: 0.6427\n",
      "Epoch 24/300\n",
      "248/248 [==============================] - 0s 916us/step - loss: 0.9373 - accuracy: 0.6455\n",
      "Epoch 25/300\n",
      "248/248 [==============================] - 0s 853us/step - loss: 0.9371 - accuracy: 0.6460\n",
      "Epoch 26/300\n",
      "248/248 [==============================] - 0s 865us/step - loss: 0.9355 - accuracy: 0.6445\n",
      "Epoch 27/300\n",
      "248/248 [==============================] - 0s 846us/step - loss: 0.9325 - accuracy: 0.6463\n",
      "Epoch 28/300\n",
      "248/248 [==============================] - 0s 955us/step - loss: 0.9313 - accuracy: 0.6461\n",
      "Epoch 29/300\n",
      "248/248 [==============================] - 0s 852us/step - loss: 0.9313 - accuracy: 0.6470\n",
      "Epoch 30/300\n",
      "248/248 [==============================] - 0s 940us/step - loss: 0.9306 - accuracy: 0.6471\n",
      "Epoch 31/300\n",
      "248/248 [==============================] - 0s 852us/step - loss: 0.9291 - accuracy: 0.6479\n",
      "Epoch 32/300\n",
      "248/248 [==============================] - 0s 859us/step - loss: 0.9267 - accuracy: 0.6479\n",
      "Epoch 33/300\n",
      "248/248 [==============================] - 0s 853us/step - loss: 0.9271 - accuracy: 0.6459\n",
      "Epoch 34/300\n",
      "248/248 [==============================] - 0s 857us/step - loss: 0.9257 - accuracy: 0.6461\n",
      "Epoch 35/300\n",
      "248/248 [==============================] - 0s 839us/step - loss: 0.9242 - accuracy: 0.6480\n",
      "Epoch 36/300\n",
      "248/248 [==============================] - 0s 859us/step - loss: 0.9237 - accuracy: 0.6484\n",
      "Epoch 37/300\n",
      "248/248 [==============================] - 0s 855us/step - loss: 0.9239 - accuracy: 0.6496\n",
      "Epoch 38/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.9218 - accuracy: 0.6495\n",
      "Epoch 39/300\n",
      "248/248 [==============================] - 0s 854us/step - loss: 0.9218 - accuracy: 0.6480\n",
      "Epoch 40/300\n",
      "248/248 [==============================] - 0s 865us/step - loss: 0.9215 - accuracy: 0.6482\n",
      "Epoch 41/300\n",
      "248/248 [==============================] - 0s 851us/step - loss: 0.9216 - accuracy: 0.6484\n",
      "Epoch 42/300\n",
      "248/248 [==============================] - 0s 878us/step - loss: 0.9195 - accuracy: 0.6503\n",
      "Epoch 43/300\n",
      "248/248 [==============================] - 0s 862us/step - loss: 0.9185 - accuracy: 0.6486\n",
      "Epoch 44/300\n",
      "248/248 [==============================] - 0s 843us/step - loss: 0.9195 - accuracy: 0.6497\n",
      "Epoch 45/300\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.9174 - accuracy: 0.6496\n",
      "Epoch 46/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.9177 - accuracy: 0.6494\n",
      "Epoch 47/300\n",
      "248/248 [==============================] - 0s 858us/step - loss: 0.9159 - accuracy: 0.6483\n",
      "Epoch 48/300\n",
      "248/248 [==============================] - 0s 831us/step - loss: 0.9157 - accuracy: 0.6506\n",
      "Epoch 49/300\n",
      "248/248 [==============================] - 0s 815us/step - loss: 0.9145 - accuracy: 0.6510\n",
      "Epoch 50/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.9162 - accuracy: 0.6515\n",
      "Epoch 51/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 0.9144 - accuracy: 0.6518\n",
      "Epoch 52/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.9136 - accuracy: 0.6504\n",
      "Epoch 53/300\n",
      "248/248 [==============================] - 0s 892us/step - loss: 0.9132 - accuracy: 0.6510\n",
      "Epoch 54/300\n",
      "248/248 [==============================] - 0s 851us/step - loss: 0.9134 - accuracy: 0.6508\n",
      "Epoch 55/300\n",
      "248/248 [==============================] - 0s 829us/step - loss: 0.9120 - accuracy: 0.6522\n",
      "Epoch 56/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.9104 - accuracy: 0.6537\n",
      "Epoch 57/300\n",
      "248/248 [==============================] - 0s 841us/step - loss: 0.9105 - accuracy: 0.6512\n",
      "Epoch 58/300\n",
      "248/248 [==============================] - 0s 831us/step - loss: 0.9105 - accuracy: 0.6528\n",
      "Epoch 59/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.9100 - accuracy: 0.6538\n",
      "Epoch 60/300\n",
      "248/248 [==============================] - 0s 839us/step - loss: 0.9110 - accuracy: 0.6514\n",
      "Epoch 61/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.9084 - accuracy: 0.6528\n",
      "Epoch 62/300\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.9091 - accuracy: 0.6523\n",
      "Epoch 63/300\n",
      "248/248 [==============================] - 0s 807us/step - loss: 0.9079 - accuracy: 0.6538\n",
      "Epoch 64/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.9075 - accuracy: 0.6552\n",
      "Epoch 65/300\n",
      "248/248 [==============================] - 0s 803us/step - loss: 0.9066 - accuracy: 0.6547\n",
      "Epoch 66/300\n",
      "248/248 [==============================] - 0s 835us/step - loss: 0.9070 - accuracy: 0.6533\n",
      "Epoch 67/300\n",
      "248/248 [==============================] - 0s 819us/step - loss: 0.9053 - accuracy: 0.6549\n",
      "Epoch 68/300\n",
      "248/248 [==============================] - 0s 819us/step - loss: 0.9053 - accuracy: 0.6542\n",
      "Epoch 69/300\n",
      "248/248 [==============================] - 0s 818us/step - loss: 0.9062 - accuracy: 0.6542\n",
      "Epoch 70/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.9044 - accuracy: 0.6548\n",
      "Epoch 71/300\n",
      "248/248 [==============================] - 0s 837us/step - loss: 0.9045 - accuracy: 0.6563\n",
      "Epoch 72/300\n",
      "248/248 [==============================] - 0s 825us/step - loss: 0.9042 - accuracy: 0.6550\n",
      "Epoch 73/300\n",
      "248/248 [==============================] - 0s 813us/step - loss: 0.9032 - accuracy: 0.6551\n",
      "Epoch 74/300\n",
      "248/248 [==============================] - 0s 833us/step - loss: 0.9034 - accuracy: 0.6545\n",
      "Epoch 75/300\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.9045 - accuracy: 0.6527\n",
      "Epoch 76/300\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.9009 - accuracy: 0.6556\n",
      "Epoch 77/300\n",
      "248/248 [==============================] - 0s 829us/step - loss: 0.9019 - accuracy: 0.6554\n",
      "Epoch 78/300\n",
      "248/248 [==============================] - 0s 947us/step - loss: 0.9025 - accuracy: 0.6547\n",
      "Epoch 79/300\n",
      "248/248 [==============================] - 0s 851us/step - loss: 0.9016 - accuracy: 0.6551\n",
      "Epoch 80/300\n",
      "248/248 [==============================] - 0s 868us/step - loss: 0.9006 - accuracy: 0.6556\n",
      "Epoch 81/300\n",
      "248/248 [==============================] - 0s 835us/step - loss: 0.9014 - accuracy: 0.6552\n",
      "Epoch 82/300\n",
      "248/248 [==============================] - 0s 849us/step - loss: 0.8995 - accuracy: 0.6564\n",
      "Epoch 83/300\n",
      "248/248 [==============================] - 0s 848us/step - loss: 0.8983 - accuracy: 0.6590\n",
      "Epoch 84/300\n",
      "248/248 [==============================] - 0s 853us/step - loss: 0.8986 - accuracy: 0.6562\n",
      "Epoch 85/300\n",
      "248/248 [==============================] - 0s 856us/step - loss: 0.8982 - accuracy: 0.6565\n",
      "Epoch 86/300\n",
      "248/248 [==============================] - 0s 840us/step - loss: 0.8971 - accuracy: 0.6586\n",
      "Epoch 87/300\n",
      "248/248 [==============================] - 0s 829us/step - loss: 0.8959 - accuracy: 0.6588\n",
      "Epoch 88/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.8967 - accuracy: 0.6581\n",
      "Epoch 89/300\n",
      "248/248 [==============================] - 0s 845us/step - loss: 0.8975 - accuracy: 0.6554\n",
      "Epoch 90/300\n",
      "248/248 [==============================] - 0s 840us/step - loss: 0.8963 - accuracy: 0.6580\n",
      "Epoch 91/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8947 - accuracy: 0.6574\n",
      "Epoch 92/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.8953 - accuracy: 0.6592\n",
      "Epoch 93/300\n",
      "248/248 [==============================] - 0s 821us/step - loss: 0.8944 - accuracy: 0.6566\n",
      "Epoch 94/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8936 - accuracy: 0.6590\n",
      "Epoch 95/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8934 - accuracy: 0.6587\n",
      "Epoch 96/300\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.8925 - accuracy: 0.6582\n",
      "Epoch 97/300\n",
      "248/248 [==============================] - 0s 819us/step - loss: 0.8925 - accuracy: 0.6590\n",
      "Epoch 98/300\n",
      "248/248 [==============================] - 0s 893us/step - loss: 0.8925 - accuracy: 0.6585\n",
      "Epoch 99/300\n",
      "248/248 [==============================] - 0s 847us/step - loss: 0.8918 - accuracy: 0.6581\n",
      "Epoch 100/300\n",
      "248/248 [==============================] - 0s 839us/step - loss: 0.8909 - accuracy: 0.6590\n",
      "Epoch 101/300\n",
      "248/248 [==============================] - 0s 825us/step - loss: 0.8907 - accuracy: 0.6596\n",
      "Epoch 102/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8904 - accuracy: 0.6590\n",
      "Epoch 103/300\n",
      "248/248 [==============================] - 0s 801us/step - loss: 0.8896 - accuracy: 0.6598\n",
      "Epoch 104/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8893 - accuracy: 0.6597\n",
      "Epoch 105/300\n",
      "248/248 [==============================] - 0s 865us/step - loss: 0.8889 - accuracy: 0.6595\n",
      "Epoch 106/300\n",
      "248/248 [==============================] - 0s 843us/step - loss: 0.8893 - accuracy: 0.6588\n",
      "Epoch 107/300\n",
      "248/248 [==============================] - 0s 875us/step - loss: 0.8876 - accuracy: 0.6592\n",
      "Epoch 108/300\n",
      "248/248 [==============================] - 0s 831us/step - loss: 0.8876 - accuracy: 0.6614\n",
      "Epoch 109/300\n",
      "248/248 [==============================] - 0s 839us/step - loss: 0.8881 - accuracy: 0.6584\n",
      "Epoch 110/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8862 - accuracy: 0.6613\n",
      "Epoch 111/300\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.8874 - accuracy: 0.6597\n",
      "Epoch 112/300\n",
      "248/248 [==============================] - 0s 833us/step - loss: 0.8853 - accuracy: 0.6611\n",
      "Epoch 113/300\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.8859 - accuracy: 0.6595\n",
      "Epoch 114/300\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.8860 - accuracy: 0.6609\n",
      "Epoch 115/300\n",
      "248/248 [==============================] - 0s 831us/step - loss: 0.8850 - accuracy: 0.6595\n",
      "Epoch 116/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8828 - accuracy: 0.6628\n",
      "Epoch 117/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.8828 - accuracy: 0.6624\n",
      "Epoch 118/300\n",
      "248/248 [==============================] - 0s 910us/step - loss: 0.8826 - accuracy: 0.6634\n",
      "Epoch 119/300\n",
      "248/248 [==============================] - 0s 819us/step - loss: 0.8821 - accuracy: 0.6634\n",
      "Epoch 120/300\n",
      "248/248 [==============================] - 0s 787us/step - loss: 0.8827 - accuracy: 0.6642\n",
      "Epoch 121/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8806 - accuracy: 0.6636\n",
      "Epoch 122/300\n",
      "248/248 [==============================] - 0s 846us/step - loss: 0.8815 - accuracy: 0.6650\n",
      "Epoch 123/300\n",
      "248/248 [==============================] - 0s 852us/step - loss: 0.8816 - accuracy: 0.6630\n",
      "Epoch 124/300\n",
      "248/248 [==============================] - 0s 856us/step - loss: 0.8799 - accuracy: 0.6632\n",
      "Epoch 125/300\n",
      "248/248 [==============================] - 0s 848us/step - loss: 0.8798 - accuracy: 0.6647\n",
      "Epoch 126/300\n",
      "248/248 [==============================] - 0s 846us/step - loss: 0.8797 - accuracy: 0.6622\n",
      "Epoch 127/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8792 - accuracy: 0.6633\n",
      "Epoch 128/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8777 - accuracy: 0.6657\n",
      "Epoch 129/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8786 - accuracy: 0.6627\n",
      "Epoch 130/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 0.8770 - accuracy: 0.6637\n",
      "Epoch 131/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8770 - accuracy: 0.6650\n",
      "Epoch 132/300\n",
      "248/248 [==============================] - 0s 871us/step - loss: 0.8760 - accuracy: 0.6657\n",
      "Epoch 133/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.8779 - accuracy: 0.6637\n",
      "Epoch 134/300\n",
      "248/248 [==============================] - 0s 840us/step - loss: 0.8754 - accuracy: 0.6634\n",
      "Epoch 135/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8753 - accuracy: 0.6640\n",
      "Epoch 136/300\n",
      "248/248 [==============================] - 0s 931us/step - loss: 0.8737 - accuracy: 0.6647\n",
      "Epoch 137/300\n",
      "248/248 [==============================] - 0s 860us/step - loss: 0.8739 - accuracy: 0.6650\n",
      "Epoch 138/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8741 - accuracy: 0.6645\n",
      "Epoch 139/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.8728 - accuracy: 0.6654\n",
      "Epoch 140/300\n",
      "248/248 [==============================] - 0s 818us/step - loss: 0.8716 - accuracy: 0.6652\n",
      "Epoch 141/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.8721 - accuracy: 0.6673\n",
      "Epoch 142/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.8717 - accuracy: 0.6652\n",
      "Epoch 143/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8709 - accuracy: 0.6674\n",
      "Epoch 144/300\n",
      "248/248 [==============================] - 0s 818us/step - loss: 0.8708 - accuracy: 0.6666\n",
      "Epoch 145/300\n",
      "248/248 [==============================] - 0s 846us/step - loss: 0.8702 - accuracy: 0.6663\n",
      "Epoch 146/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8701 - accuracy: 0.6661\n",
      "Epoch 147/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.8692 - accuracy: 0.6688\n",
      "Epoch 148/300\n",
      "248/248 [==============================] - 0s 840us/step - loss: 0.8697 - accuracy: 0.6664\n",
      "Epoch 149/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.8688 - accuracy: 0.6697\n",
      "Epoch 150/300\n",
      "248/248 [==============================] - 0s 840us/step - loss: 0.8688 - accuracy: 0.6666\n",
      "Epoch 151/300\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.8681 - accuracy: 0.6694\n",
      "Epoch 152/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8680 - accuracy: 0.6678\n",
      "Epoch 153/300\n",
      "248/248 [==============================] - 0s 905us/step - loss: 0.8668 - accuracy: 0.6675\n",
      "Epoch 154/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8677 - accuracy: 0.6674\n",
      "Epoch 155/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8674 - accuracy: 0.6677\n",
      "Epoch 156/300\n",
      "248/248 [==============================] - 0s 887us/step - loss: 0.8673 - accuracy: 0.6666\n",
      "Epoch 157/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.8657 - accuracy: 0.6682\n",
      "Epoch 158/300\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.8654 - accuracy: 0.6678\n",
      "Epoch 159/300\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.8655 - accuracy: 0.6674\n",
      "Epoch 160/300\n",
      "248/248 [==============================] - 0s 806us/step - loss: 0.8650 - accuracy: 0.6687\n",
      "Epoch 161/300\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.8636 - accuracy: 0.6700\n",
      "Epoch 162/300\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.8639 - accuracy: 0.6687\n",
      "Epoch 163/300\n",
      "248/248 [==============================] - 0s 810us/step - loss: 0.8638 - accuracy: 0.6700\n",
      "Epoch 164/300\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.8630 - accuracy: 0.6682\n",
      "Epoch 165/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.8629 - accuracy: 0.6687\n",
      "Epoch 166/300\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.8623 - accuracy: 0.6694\n",
      "Epoch 167/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.8627 - accuracy: 0.6695\n",
      "Epoch 168/300\n",
      "248/248 [==============================] - 0s 806us/step - loss: 0.8629 - accuracy: 0.6681\n",
      "Epoch 169/300\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.8620 - accuracy: 0.6707\n",
      "Epoch 170/300\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.8617 - accuracy: 0.6698\n",
      "Epoch 171/300\n",
      "248/248 [==============================] - 0s 909us/step - loss: 0.8605 - accuracy: 0.6720\n",
      "Epoch 172/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8615 - accuracy: 0.6692\n",
      "Epoch 173/300\n",
      "248/248 [==============================] - 0s 848us/step - loss: 0.8626 - accuracy: 0.6706\n",
      "Epoch 174/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8596 - accuracy: 0.6713\n",
      "Epoch 175/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8596 - accuracy: 0.6707\n",
      "Epoch 176/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.8586 - accuracy: 0.6715\n",
      "Epoch 177/300\n",
      "248/248 [==============================] - 0s 877us/step - loss: 0.8590 - accuracy: 0.6709\n",
      "Epoch 178/300\n",
      "248/248 [==============================] - 0s 854us/step - loss: 0.8589 - accuracy: 0.6719\n",
      "Epoch 179/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8587 - accuracy: 0.6705\n",
      "Epoch 180/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8578 - accuracy: 0.6726\n",
      "Epoch 181/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.8579 - accuracy: 0.6706\n",
      "Epoch 182/300\n",
      "248/248 [==============================] - 0s 854us/step - loss: 0.8572 - accuracy: 0.6726\n",
      "Epoch 183/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8568 - accuracy: 0.6714\n",
      "Epoch 184/300\n",
      "248/248 [==============================] - 0s 854us/step - loss: 0.8579 - accuracy: 0.6716\n",
      "Epoch 185/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.8567 - accuracy: 0.6715\n",
      "Epoch 186/300\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.8555 - accuracy: 0.6718\n",
      "Epoch 187/300\n",
      "248/248 [==============================] - 0s 915us/step - loss: 0.8562 - accuracy: 0.6731\n",
      "Epoch 188/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 0.8558 - accuracy: 0.6740\n",
      "Epoch 189/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.8558 - accuracy: 0.6719\n",
      "Epoch 190/300\n",
      "248/248 [==============================] - 0s 846us/step - loss: 0.8553 - accuracy: 0.6745\n",
      "Epoch 191/300\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.8555 - accuracy: 0.6731\n",
      "Epoch 192/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8546 - accuracy: 0.6738\n",
      "Epoch 193/300\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.8545 - accuracy: 0.6742\n",
      "Epoch 194/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.8536 - accuracy: 0.6740\n",
      "Epoch 195/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8538 - accuracy: 0.6718\n",
      "Epoch 196/300\n",
      "248/248 [==============================] - 0s 871us/step - loss: 0.8533 - accuracy: 0.6733\n",
      "Epoch 197/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8524 - accuracy: 0.6737\n",
      "Epoch 198/300\n",
      "248/248 [==============================] - 0s 804us/step - loss: 0.8533 - accuracy: 0.6728\n",
      "Epoch 199/300\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.8533 - accuracy: 0.6754\n",
      "Epoch 200/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8527 - accuracy: 0.6735\n",
      "Epoch 201/300\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 0.8534 - accuracy: 0.6737\n",
      "Epoch 202/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8509 - accuracy: 0.6738\n",
      "Epoch 203/300\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.8510 - accuracy: 0.6746\n",
      "Epoch 204/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8512 - accuracy: 0.6750\n",
      "Epoch 205/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.8502 - accuracy: 0.6759\n",
      "Epoch 206/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8502 - accuracy: 0.6765\n",
      "Epoch 207/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8498 - accuracy: 0.6746\n",
      "Epoch 208/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.8510 - accuracy: 0.6729\n",
      "Epoch 209/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 0.8497 - accuracy: 0.6753\n",
      "Epoch 210/300\n",
      "248/248 [==============================] - 0s 848us/step - loss: 0.8495 - accuracy: 0.6742\n",
      "Epoch 211/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 0.8506 - accuracy: 0.6741\n",
      "Epoch 212/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.8486 - accuracy: 0.6741\n",
      "Epoch 213/300\n",
      "248/248 [==============================] - 0s 852us/step - loss: 0.8499 - accuracy: 0.6742\n",
      "Epoch 214/300\n",
      "248/248 [==============================] - 0s 856us/step - loss: 0.8478 - accuracy: 0.6752\n",
      "Epoch 215/300\n",
      "248/248 [==============================] - 0s 942us/step - loss: 0.8481 - accuracy: 0.6751\n",
      "Epoch 216/300\n",
      "248/248 [==============================] - 0s 857us/step - loss: 0.8481 - accuracy: 0.6759\n",
      "Epoch 217/300\n",
      "248/248 [==============================] - 0s 864us/step - loss: 0.8474 - accuracy: 0.6753\n",
      "Epoch 218/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8475 - accuracy: 0.6765\n",
      "Epoch 219/300\n",
      "248/248 [==============================] - 0s 869us/step - loss: 0.8476 - accuracy: 0.6748\n",
      "Epoch 220/300\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.8476 - accuracy: 0.6767\n",
      "Epoch 221/300\n",
      "248/248 [==============================] - 0s 849us/step - loss: 0.8465 - accuracy: 0.6768\n",
      "Epoch 222/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8460 - accuracy: 0.6755\n",
      "Epoch 223/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8456 - accuracy: 0.6764\n",
      "Epoch 224/300\n",
      "248/248 [==============================] - 0s 854us/step - loss: 0.8466 - accuracy: 0.6764\n",
      "Epoch 225/300\n",
      "248/248 [==============================] - 0s 855us/step - loss: 0.8465 - accuracy: 0.6769\n",
      "Epoch 226/300\n",
      "248/248 [==============================] - 0s 863us/step - loss: 0.8458 - accuracy: 0.6747\n",
      "Epoch 227/300\n",
      "248/248 [==============================] - 0s 835us/step - loss: 0.8455 - accuracy: 0.6747\n",
      "Epoch 228/300\n",
      "248/248 [==============================] - 0s 914us/step - loss: 0.8444 - accuracy: 0.6783\n",
      "Epoch 229/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8453 - accuracy: 0.6767\n",
      "Epoch 230/300\n",
      "248/248 [==============================] - 0s 914us/step - loss: 0.8441 - accuracy: 0.6778\n",
      "Epoch 231/300\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.8451 - accuracy: 0.6754\n",
      "Epoch 232/300\n",
      "248/248 [==============================] - 0s 834us/step - loss: 0.8440 - accuracy: 0.6781\n",
      "Epoch 233/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.8426 - accuracy: 0.6763\n",
      "Epoch 234/300\n",
      "248/248 [==============================] - 0s 846us/step - loss: 0.8429 - accuracy: 0.6755\n",
      "Epoch 235/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8432 - accuracy: 0.6761\n",
      "Epoch 236/300\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.8426 - accuracy: 0.6788\n",
      "Epoch 237/300\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.8415 - accuracy: 0.6771\n",
      "Epoch 238/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8408 - accuracy: 0.6781\n",
      "Epoch 239/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.8425 - accuracy: 0.6769\n",
      "Epoch 240/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8429 - accuracy: 0.6767\n",
      "Epoch 241/300\n",
      "248/248 [==============================] - 0s 800us/step - loss: 0.8415 - accuracy: 0.6770\n",
      "Epoch 242/300\n",
      "248/248 [==============================] - 0s 842us/step - loss: 0.8408 - accuracy: 0.6772\n",
      "Epoch 243/300\n",
      "248/248 [==============================] - 0s 954us/step - loss: 0.8400 - accuracy: 0.6777\n",
      "Epoch 244/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8410 - accuracy: 0.6781\n",
      "Epoch 245/300\n",
      "248/248 [==============================] - 0s 798us/step - loss: 0.8395 - accuracy: 0.6767\n",
      "Epoch 246/300\n",
      "248/248 [==============================] - 0s 783us/step - loss: 0.8412 - accuracy: 0.6766\n",
      "Epoch 247/300\n",
      "248/248 [==============================] - 0s 794us/step - loss: 0.8398 - accuracy: 0.6793\n",
      "Epoch 248/300\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.8414 - accuracy: 0.6773\n",
      "Epoch 249/300\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.8391 - accuracy: 0.6765\n",
      "Epoch 250/300\n",
      "248/248 [==============================] - 0s 800us/step - loss: 0.8399 - accuracy: 0.6784\n",
      "Epoch 251/300\n",
      "248/248 [==============================] - 0s 796us/step - loss: 0.8376 - accuracy: 0.6795\n",
      "Epoch 252/300\n",
      "248/248 [==============================] - 0s 802us/step - loss: 0.8390 - accuracy: 0.6793\n",
      "Epoch 253/300\n",
      "248/248 [==============================] - 0s 804us/step - loss: 0.8384 - accuracy: 0.6770\n",
      "Epoch 254/300\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.8388 - accuracy: 0.6752\n",
      "Epoch 255/300\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.8385 - accuracy: 0.6787\n",
      "Epoch 256/300\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.8373 - accuracy: 0.6790\n",
      "Epoch 257/300\n",
      "248/248 [==============================] - 0s 891us/step - loss: 0.8374 - accuracy: 0.6795\n",
      "Epoch 258/300\n",
      "248/248 [==============================] - 0s 901us/step - loss: 0.8381 - accuracy: 0.6788\n",
      "Epoch 259/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8379 - accuracy: 0.6771\n",
      "Epoch 260/300\n",
      "248/248 [==============================] - 0s 864us/step - loss: 0.8378 - accuracy: 0.6792\n",
      "Epoch 261/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8388 - accuracy: 0.6760\n",
      "Epoch 262/300\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.8368 - accuracy: 0.6781\n",
      "Epoch 263/300\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.8363 - accuracy: 0.6778\n",
      "Epoch 264/300\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.8351 - accuracy: 0.6780\n",
      "Epoch 265/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.8351 - accuracy: 0.6793\n",
      "Epoch 266/300\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.8355 - accuracy: 0.6791\n",
      "Epoch 267/300\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.8342 - accuracy: 0.6813\n",
      "Epoch 268/300\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.8347 - accuracy: 0.6808\n",
      "Epoch 269/300\n",
      "248/248 [==============================] - 0s 830us/step - loss: 0.8345 - accuracy: 0.6812\n",
      "Epoch 270/300\n",
      "248/248 [==============================] - 0s 863us/step - loss: 0.8342 - accuracy: 0.6806\n",
      "Epoch 271/300\n",
      "248/248 [==============================] - 0s 829us/step - loss: 0.8342 - accuracy: 0.6795\n",
      "Epoch 272/300\n",
      "248/248 [==============================] - 0s 874us/step - loss: 0.8335 - accuracy: 0.6803\n",
      "Epoch 273/300\n",
      "248/248 [==============================] - 0s 833us/step - loss: 0.8346 - accuracy: 0.6791\n",
      "Epoch 274/300\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.8342 - accuracy: 0.6813\n",
      "Epoch 275/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.8342 - accuracy: 0.6798\n",
      "Epoch 276/300\n",
      "248/248 [==============================] - 0s 825us/step - loss: 0.8333 - accuracy: 0.6798\n",
      "Epoch 277/300\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.8333 - accuracy: 0.6785\n",
      "Epoch 278/300\n",
      "248/248 [==============================] - 0s 822us/step - loss: 0.8331 - accuracy: 0.6808\n",
      "Epoch 279/300\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.8329 - accuracy: 0.6793\n",
      "Epoch 280/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8325 - accuracy: 0.6802\n",
      "Epoch 281/300\n",
      "248/248 [==============================] - 0s 847us/step - loss: 0.8313 - accuracy: 0.6810\n",
      "Epoch 282/300\n",
      "248/248 [==============================] - 0s 904us/step - loss: 0.8312 - accuracy: 0.6801\n",
      "Epoch 283/300\n",
      "248/248 [==============================] - 0s 841us/step - loss: 0.8306 - accuracy: 0.6816\n",
      "Epoch 284/300\n",
      "248/248 [==============================] - 0s 836us/step - loss: 0.8323 - accuracy: 0.6817\n",
      "Epoch 285/300\n",
      "248/248 [==============================] - 0s 867us/step - loss: 0.8319 - accuracy: 0.6799\n",
      "Epoch 286/300\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.8313 - accuracy: 0.6807\n",
      "Epoch 287/300\n",
      "248/248 [==============================] - 0s 849us/step - loss: 0.8316 - accuracy: 0.6825\n",
      "Epoch 288/300\n",
      "248/248 [==============================] - 0s 835us/step - loss: 0.8300 - accuracy: 0.6823\n",
      "Epoch 289/300\n",
      "248/248 [==============================] - 0s 811us/step - loss: 0.8306 - accuracy: 0.6807\n",
      "Epoch 290/300\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.8289 - accuracy: 0.6830\n",
      "Epoch 291/300\n",
      "248/248 [==============================] - 0s 839us/step - loss: 0.8305 - accuracy: 0.6793\n",
      "Epoch 292/300\n",
      "248/248 [==============================] - 0s 833us/step - loss: 0.8293 - accuracy: 0.6820\n",
      "Epoch 293/300\n",
      "248/248 [==============================] - 0s 888us/step - loss: 0.8296 - accuracy: 0.6800\n",
      "Epoch 294/300\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.8291 - accuracy: 0.6794\n",
      "Epoch 295/300\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.8307 - accuracy: 0.6821\n",
      "Epoch 296/300\n",
      "248/248 [==============================] - 0s 826us/step - loss: 0.8271 - accuracy: 0.6836\n",
      "Epoch 297/300\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.8284 - accuracy: 0.6826\n",
      "Epoch 298/300\n",
      "248/248 [==============================] - 0s 874us/step - loss: 0.8280 - accuracy: 0.6828\n",
      "Epoch 299/300\n",
      "248/248 [==============================] - 0s 833us/step - loss: 0.8287 - accuracy: 0.6827\n",
      "Epoch 300/300\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.8280 - accuracy: 0.6826\n",
      "124/124 [==============================] - 0s 695us/step\n",
      "(3956, 6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  atmosphere       0.42      0.30      0.35       277\n",
      "        food       0.73      0.77      0.75      1641\n",
      "     hygiene       0.40      0.19      0.25       107\n",
      "    location       0.49      0.36      0.41        94\n",
      "        none       0.51      0.57      0.54       763\n",
      "     service       0.66      0.64      0.65      1074\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      3956\n",
      "   macro avg       0.53      0.47      0.49      3956\n",
      "weighted avg       0.63      0.64      0.63      3956\n",
      " samples avg       0.64      0.64      0.64      3956\n",
      "\n",
      "Accuracy: 0.6382709807886754\n"
     ]
    }
   ],
   "source": [
    "cluster_lda(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc, lemmatized=False, remove_stopword=False,\n",
    "             remove_punct=True, pos_tag=False):\n",
    "    spacy_doc = nlp(doc)\n",
    "    tokens = []\n",
    "    for token in spacy_doc:\n",
    "        if remove_punct and token.is_punct:\n",
    "            continue\n",
    "        if remove_stopword and token.is_stop:\n",
    "            continue\n",
    "        if not token.text.strip():\n",
    "            continue\n",
    "        words = token.lemma_ if lemmatized else token.text\n",
    "        if pos_tag:\n",
    "            tokens.append((words.lower(), token.pos_))\n",
    "        else:\n",
    "            tokens.append(words.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_doc_tokens(tokens):\n",
    "    token_count = {}\n",
    "    for token in tokens:\n",
    "        token_count[token] = token_count.get(token, 0) + 1\n",
    "    return token_count\n",
    "\n",
    "def compute_tfidf(tokenized_docs):\n",
    "    docs_tokens = {idx: get_doc_tokens(doc)for idx, doc in enumerate(tokenized_docs)}\n",
    "    dtm = pd.DataFrame.from_dict(docs_tokens, orient=\"index\")\n",
    "    dtm = dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis=0)\n",
    "    tf = dtm.values\n",
    "    doc_len = tf.sum(axis=1, keepdims=True)\n",
    "    tf = np.divide(tf, doc_len)\n",
    "    df = np.where(tf > 0, 1, 0)\n",
    "    smoothed_idf = np.log(np.divide(len(tokenized_docs) + 1, np.sum(df, axis=0) + 1)) + 1\n",
    "    smoothed_tf_idf = tf * smoothed_idf\n",
    "    terms = dtm.columns.to_list()\n",
    "    return smoothed_tf_idf,terms\n",
    "\n",
    "def make_tfidf(datas,onlyNoun=False):\n",
    "    tokenslist=[]\n",
    "    for data in datas:\n",
    "        tokens=tokenize(data,lemmatized=True, remove_stopword=True,pos_tag=onlyNoun)\n",
    "        if not onlyNoun:\n",
    "            tokenslist.append(tokens)\n",
    "        else:\n",
    "            newtokens=[]\n",
    "            for (token,tag) in tokens:\n",
    "                if tag==\"NOUN\" or tag==\"PROPN\":\n",
    "                    newtokens.append(token)\n",
    "            tokenslist.append(newtokens)\n",
    "    return compute_tfidf(tokenslist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def ANN_classificate(train_text, test_text, train_label, test_label):\n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,ngram_range=(1,1),lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text).toarray()\n",
    "    test_dtm=tfidf.transform(test_text).toarray()\n",
    "    lb = LabelBinarizer()            \n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 6000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 8000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    #train_dtm,train_label = sm.fit_resample(train_dtm, train_label)\n",
    "    train_y=lb.fit_transform(train_label)\n",
    "    test_y=lb.transform(test_label)\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(train_dtm.shape[1],)),\n",
    "    Dense(32, activation='sigmoid'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(6, activation='softmax')  \n",
    "])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled=scaler.fit_transform(train_dtm)\n",
    "    X_test_scaled=scaler.fit_transform(test_dtm)\n",
    "\n",
    "    model.fit(X_train_scaled, train_y, epochs=200, batch_size=64)\n",
    "\n",
    "    y_pred=model.predict(X_test_scaled)\n",
    "\n",
    "    y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_pred_binary = np.zeros_like(y_pred)\n",
    "    for i, class_index in enumerate(y_pred_argmax):\n",
    "        y_pred_binary[i, class_index] = 1\n",
    "\n",
    "\n",
    "\n",
    "    print(y_pred_binary.shape)\n",
    "    report=classification_report(test_y,y_pred_binary)\n",
    "    print(report)\n",
    "    print(\"Accuracy:\", accuracy_score(test_y, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "248/248 [==============================] - 1s 885us/step - loss: 1.4209 - accuracy: 0.4056\n",
      "Epoch 2/200\n",
      "248/248 [==============================] - 0s 848us/step - loss: 1.0744 - accuracy: 0.6097\n",
      "Epoch 3/200\n",
      "248/248 [==============================] - 0s 833us/step - loss: 0.9433 - accuracy: 0.6594\n",
      "Epoch 4/200\n",
      "248/248 [==============================] - 0s 795us/step - loss: 0.8902 - accuracy: 0.6803\n",
      "Epoch 5/200\n",
      "248/248 [==============================] - 0s 773us/step - loss: 0.8592 - accuracy: 0.6877\n",
      "Epoch 6/200\n",
      "248/248 [==============================] - 0s 845us/step - loss: 0.8371 - accuracy: 0.6973\n",
      "Epoch 7/200\n",
      "248/248 [==============================] - 0s 841us/step - loss: 0.8239 - accuracy: 0.7025\n",
      "Epoch 8/200\n",
      "248/248 [==============================] - 0s 821us/step - loss: 0.8126 - accuracy: 0.7100\n",
      "Epoch 9/200\n",
      "248/248 [==============================] - 0s 775us/step - loss: 0.8037 - accuracy: 0.7149\n",
      "Epoch 10/200\n",
      "248/248 [==============================] - 0s 786us/step - loss: 0.7957 - accuracy: 0.7168\n",
      "Epoch 11/200\n",
      "248/248 [==============================] - 0s 800us/step - loss: 0.7905 - accuracy: 0.7154\n",
      "Epoch 12/200\n",
      "248/248 [==============================] - 0s 804us/step - loss: 0.7853 - accuracy: 0.7154\n",
      "Epoch 13/200\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.7818 - accuracy: 0.7180\n",
      "Epoch 14/200\n",
      "248/248 [==============================] - 0s 792us/step - loss: 0.7808 - accuracy: 0.7175\n",
      "Epoch 15/200\n",
      "248/248 [==============================] - 0s 875us/step - loss: 0.7761 - accuracy: 0.7178\n",
      "Epoch 16/200\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.7730 - accuracy: 0.7183\n",
      "Epoch 17/200\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.7717 - accuracy: 0.7178\n",
      "Epoch 18/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.7709 - accuracy: 0.7189\n",
      "Epoch 19/200\n",
      "248/248 [==============================] - 0s 798us/step - loss: 0.7688 - accuracy: 0.7189\n",
      "Epoch 20/200\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.7674 - accuracy: 0.7194\n",
      "Epoch 21/200\n",
      "248/248 [==============================] - 0s 806us/step - loss: 0.7651 - accuracy: 0.7209\n",
      "Epoch 22/200\n",
      "248/248 [==============================] - 0s 815us/step - loss: 0.7635 - accuracy: 0.7201\n",
      "Epoch 23/200\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.7612 - accuracy: 0.7211\n",
      "Epoch 24/200\n",
      "248/248 [==============================] - 0s 844us/step - loss: 0.7590 - accuracy: 0.7224\n",
      "Epoch 25/200\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.7596 - accuracy: 0.7228\n",
      "Epoch 26/200\n",
      "248/248 [==============================] - 0s 877us/step - loss: 0.7570 - accuracy: 0.7228\n",
      "Epoch 27/200\n",
      "248/248 [==============================] - 0s 897us/step - loss: 0.7565 - accuracy: 0.7222\n",
      "Epoch 28/200\n",
      "248/248 [==============================] - 0s 869us/step - loss: 0.7548 - accuracy: 0.7237\n",
      "Epoch 29/200\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.7537 - accuracy: 0.7245\n",
      "Epoch 30/200\n",
      "248/248 [==============================] - 0s 879us/step - loss: 0.7528 - accuracy: 0.7229\n",
      "Epoch 31/200\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.7513 - accuracy: 0.7240\n",
      "Epoch 32/200\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.7497 - accuracy: 0.7262\n",
      "Epoch 33/200\n",
      "248/248 [==============================] - 0s 843us/step - loss: 0.7495 - accuracy: 0.7248\n",
      "Epoch 34/200\n",
      "248/248 [==============================] - 0s 866us/step - loss: 0.7475 - accuracy: 0.7245\n",
      "Epoch 35/200\n",
      "248/248 [==============================] - 0s 924us/step - loss: 0.7461 - accuracy: 0.7276\n",
      "Epoch 36/200\n",
      "248/248 [==============================] - 0s 872us/step - loss: 0.7469 - accuracy: 0.7265\n",
      "Epoch 37/200\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.7465 - accuracy: 0.7257\n",
      "Epoch 38/200\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.7439 - accuracy: 0.7257\n",
      "Epoch 39/200\n",
      "248/248 [==============================] - 0s 807us/step - loss: 0.7440 - accuracy: 0.7259\n",
      "Epoch 40/200\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.7427 - accuracy: 0.7272\n",
      "Epoch 41/200\n",
      "248/248 [==============================] - 0s 806us/step - loss: 0.7420 - accuracy: 0.7295\n",
      "Epoch 42/200\n",
      "248/248 [==============================] - 0s 911us/step - loss: 0.7414 - accuracy: 0.7257\n",
      "Epoch 43/200\n",
      "248/248 [==============================] - 0s 807us/step - loss: 0.7394 - accuracy: 0.7278\n",
      "Epoch 44/200\n",
      "248/248 [==============================] - 0s 892us/step - loss: 0.7401 - accuracy: 0.7277\n",
      "Epoch 45/200\n",
      "248/248 [==============================] - 0s 891us/step - loss: 0.7388 - accuracy: 0.7295\n",
      "Epoch 46/200\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.7371 - accuracy: 0.7271\n",
      "Epoch 47/200\n",
      "248/248 [==============================] - 0s 810us/step - loss: 0.7369 - accuracy: 0.7288\n",
      "Epoch 48/200\n",
      "248/248 [==============================] - 0s 800us/step - loss: 0.7358 - accuracy: 0.7302\n",
      "Epoch 49/200\n",
      "248/248 [==============================] - 0s 811us/step - loss: 0.7342 - accuracy: 0.7277\n",
      "Epoch 50/200\n",
      "248/248 [==============================] - 0s 772us/step - loss: 0.7340 - accuracy: 0.7298\n",
      "Epoch 51/200\n",
      "248/248 [==============================] - 0s 784us/step - loss: 0.7332 - accuracy: 0.7286\n",
      "Epoch 52/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.7334 - accuracy: 0.7289\n",
      "Epoch 53/200\n",
      "248/248 [==============================] - 0s 810us/step - loss: 0.7320 - accuracy: 0.7311\n",
      "Epoch 54/200\n",
      "248/248 [==============================] - 0s 801us/step - loss: 0.7302 - accuracy: 0.7323\n",
      "Epoch 55/200\n",
      "248/248 [==============================] - 0s 802us/step - loss: 0.7308 - accuracy: 0.7301\n",
      "Epoch 56/200\n",
      "248/248 [==============================] - 0s 815us/step - loss: 0.7304 - accuracy: 0.7287\n",
      "Epoch 57/200\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.7284 - accuracy: 0.7320\n",
      "Epoch 58/200\n",
      "248/248 [==============================] - 0s 801us/step - loss: 0.7280 - accuracy: 0.7298\n",
      "Epoch 59/200\n",
      "248/248 [==============================] - 0s 836us/step - loss: 0.7268 - accuracy: 0.7320\n",
      "Epoch 60/200\n",
      "248/248 [==============================] - 0s 825us/step - loss: 0.7279 - accuracy: 0.7317\n",
      "Epoch 61/200\n",
      "248/248 [==============================] - 0s 817us/step - loss: 0.7267 - accuracy: 0.7329\n",
      "Epoch 62/200\n",
      "248/248 [==============================] - 0s 890us/step - loss: 0.7254 - accuracy: 0.7320\n",
      "Epoch 63/200\n",
      "248/248 [==============================] - 0s 817us/step - loss: 0.7236 - accuracy: 0.7333\n",
      "Epoch 64/200\n",
      "248/248 [==============================] - 0s 817us/step - loss: 0.7232 - accuracy: 0.7339\n",
      "Epoch 65/200\n",
      "248/248 [==============================] - 0s 797us/step - loss: 0.7228 - accuracy: 0.7343\n",
      "Epoch 66/200\n",
      "248/248 [==============================] - 0s 793us/step - loss: 0.7227 - accuracy: 0.7317\n",
      "Epoch 67/200\n",
      "248/248 [==============================] - 0s 817us/step - loss: 0.7229 - accuracy: 0.7346\n",
      "Epoch 68/200\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.7226 - accuracy: 0.7334\n",
      "Epoch 69/200\n",
      "248/248 [==============================] - 0s 796us/step - loss: 0.7215 - accuracy: 0.7343\n",
      "Epoch 70/200\n",
      "248/248 [==============================] - 0s 788us/step - loss: 0.7196 - accuracy: 0.7330\n",
      "Epoch 71/200\n",
      "248/248 [==============================] - 0s 792us/step - loss: 0.7200 - accuracy: 0.7336\n",
      "Epoch 72/200\n",
      "248/248 [==============================] - 0s 818us/step - loss: 0.7186 - accuracy: 0.7357\n",
      "Epoch 73/200\n",
      "248/248 [==============================] - 0s 908us/step - loss: 0.7185 - accuracy: 0.7339\n",
      "Epoch 74/200\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.7175 - accuracy: 0.7338\n",
      "Epoch 75/200\n",
      "248/248 [==============================] - 0s 803us/step - loss: 0.7164 - accuracy: 0.7330\n",
      "Epoch 76/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.7165 - accuracy: 0.7361\n",
      "Epoch 77/200\n",
      "248/248 [==============================] - 0s 813us/step - loss: 0.7152 - accuracy: 0.7355\n",
      "Epoch 78/200\n",
      "248/248 [==============================] - 0s 813us/step - loss: 0.7151 - accuracy: 0.7369\n",
      "Epoch 79/200\n",
      "248/248 [==============================] - 0s 791us/step - loss: 0.7141 - accuracy: 0.7367\n",
      "Epoch 80/200\n",
      "248/248 [==============================] - 0s 800us/step - loss: 0.7143 - accuracy: 0.7363\n",
      "Epoch 81/200\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.7135 - accuracy: 0.7365\n",
      "Epoch 82/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.7132 - accuracy: 0.7378\n",
      "Epoch 83/200\n",
      "248/248 [==============================] - 0s 785us/step - loss: 0.7124 - accuracy: 0.7380\n",
      "Epoch 84/200\n",
      "248/248 [==============================] - 0s 794us/step - loss: 0.7117 - accuracy: 0.7361\n",
      "Epoch 85/200\n",
      "248/248 [==============================] - 0s 811us/step - loss: 0.7113 - accuracy: 0.7377\n",
      "Epoch 86/200\n",
      "248/248 [==============================] - 0s 796us/step - loss: 0.7112 - accuracy: 0.7379\n",
      "Epoch 87/200\n",
      "248/248 [==============================] - 0s 810us/step - loss: 0.7101 - accuracy: 0.7368\n",
      "Epoch 88/200\n",
      "248/248 [==============================] - 0s 895us/step - loss: 0.7091 - accuracy: 0.7380\n",
      "Epoch 89/200\n",
      "248/248 [==============================] - 0s 799us/step - loss: 0.7086 - accuracy: 0.7363\n",
      "Epoch 90/200\n",
      "248/248 [==============================] - 0s 779us/step - loss: 0.7070 - accuracy: 0.7376\n",
      "Epoch 91/200\n",
      "248/248 [==============================] - 0s 784us/step - loss: 0.7077 - accuracy: 0.7390\n",
      "Epoch 92/200\n",
      "248/248 [==============================] - 0s 777us/step - loss: 0.7079 - accuracy: 0.7389\n",
      "Epoch 93/200\n",
      "248/248 [==============================] - 0s 779us/step - loss: 0.7064 - accuracy: 0.7391\n",
      "Epoch 94/200\n",
      "248/248 [==============================] - 0s 804us/step - loss: 0.7049 - accuracy: 0.7393\n",
      "Epoch 95/200\n",
      "248/248 [==============================] - 0s 795us/step - loss: 0.7039 - accuracy: 0.7380\n",
      "Epoch 96/200\n",
      "248/248 [==============================] - 0s 795us/step - loss: 0.7050 - accuracy: 0.7392\n",
      "Epoch 97/200\n",
      "248/248 [==============================] - 0s 804us/step - loss: 0.7035 - accuracy: 0.7389\n",
      "Epoch 98/200\n",
      "248/248 [==============================] - 0s 796us/step - loss: 0.7038 - accuracy: 0.7398\n",
      "Epoch 99/200\n",
      "248/248 [==============================] - 0s 798us/step - loss: 0.7029 - accuracy: 0.7408\n",
      "Epoch 100/200\n",
      "248/248 [==============================] - 0s 783us/step - loss: 0.7020 - accuracy: 0.7408\n",
      "Epoch 101/200\n",
      "248/248 [==============================] - 0s 787us/step - loss: 0.7013 - accuracy: 0.7418\n",
      "Epoch 102/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.7001 - accuracy: 0.7444\n",
      "Epoch 103/200\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.7001 - accuracy: 0.7416\n",
      "Epoch 104/200\n",
      "248/248 [==============================] - 0s 796us/step - loss: 0.6998 - accuracy: 0.7425\n",
      "Epoch 105/200\n",
      "248/248 [==============================] - 0s 780us/step - loss: 0.6992 - accuracy: 0.7430\n",
      "Epoch 106/200\n",
      "248/248 [==============================] - 0s 879us/step - loss: 0.6987 - accuracy: 0.7429\n",
      "Epoch 107/200\n",
      "248/248 [==============================] - 0s 810us/step - loss: 0.6989 - accuracy: 0.7412\n",
      "Epoch 108/200\n",
      "248/248 [==============================] - 0s 797us/step - loss: 0.6973 - accuracy: 0.7444\n",
      "Epoch 109/200\n",
      "248/248 [==============================] - 0s 788us/step - loss: 0.6966 - accuracy: 0.7441\n",
      "Epoch 110/200\n",
      "248/248 [==============================] - 0s 767us/step - loss: 0.6980 - accuracy: 0.7426\n",
      "Epoch 111/200\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.6957 - accuracy: 0.7447\n",
      "Epoch 112/200\n",
      "248/248 [==============================] - 0s 801us/step - loss: 0.6961 - accuracy: 0.7445\n",
      "Epoch 113/200\n",
      "248/248 [==============================] - 0s 794us/step - loss: 0.6948 - accuracy: 0.7461\n",
      "Epoch 114/200\n",
      "248/248 [==============================] - 0s 814us/step - loss: 0.6942 - accuracy: 0.7467\n",
      "Epoch 115/200\n",
      "248/248 [==============================] - 0s 769us/step - loss: 0.6941 - accuracy: 0.7453\n",
      "Epoch 116/200\n",
      "248/248 [==============================] - 0s 773us/step - loss: 0.6925 - accuracy: 0.7433\n",
      "Epoch 117/200\n",
      "248/248 [==============================] - 0s 797us/step - loss: 0.6920 - accuracy: 0.7449\n",
      "Epoch 118/200\n",
      "248/248 [==============================] - 0s 783us/step - loss: 0.6914 - accuracy: 0.7456\n",
      "Epoch 119/200\n",
      "248/248 [==============================] - 0s 781us/step - loss: 0.6932 - accuracy: 0.7456\n",
      "Epoch 120/200\n",
      "248/248 [==============================] - 0s 775us/step - loss: 0.6919 - accuracy: 0.7469\n",
      "Epoch 121/200\n",
      "248/248 [==============================] - 0s 768us/step - loss: 0.6893 - accuracy: 0.7475\n",
      "Epoch 122/200\n",
      "248/248 [==============================] - 0s 786us/step - loss: 0.6916 - accuracy: 0.7459\n",
      "Epoch 123/200\n",
      "248/248 [==============================] - 0s 857us/step - loss: 0.6893 - accuracy: 0.7472\n",
      "Epoch 124/200\n",
      "248/248 [==============================] - 0s 786us/step - loss: 0.6889 - accuracy: 0.7464\n",
      "Epoch 125/200\n",
      "248/248 [==============================] - 0s 777us/step - loss: 0.6883 - accuracy: 0.7472\n",
      "Epoch 126/200\n",
      "248/248 [==============================] - 0s 769us/step - loss: 0.6894 - accuracy: 0.7464\n",
      "Epoch 127/200\n",
      "248/248 [==============================] - 0s 801us/step - loss: 0.6869 - accuracy: 0.7484\n",
      "Epoch 128/200\n",
      "248/248 [==============================] - 0s 769us/step - loss: 0.6858 - accuracy: 0.7491\n",
      "Epoch 129/200\n",
      "248/248 [==============================] - 0s 791us/step - loss: 0.6870 - accuracy: 0.7487\n",
      "Epoch 130/200\n",
      "248/248 [==============================] - 0s 765us/step - loss: 0.6860 - accuracy: 0.7480\n",
      "Epoch 131/200\n",
      "248/248 [==============================] - 0s 766us/step - loss: 0.6854 - accuracy: 0.7477\n",
      "Epoch 132/200\n",
      "248/248 [==============================] - 0s 772us/step - loss: 0.6854 - accuracy: 0.7476\n",
      "Epoch 133/200\n",
      "248/248 [==============================] - 0s 863us/step - loss: 0.6844 - accuracy: 0.7487\n",
      "Epoch 134/200\n",
      "248/248 [==============================] - 0s 806us/step - loss: 0.6840 - accuracy: 0.7482\n",
      "Epoch 135/200\n",
      "248/248 [==============================] - 0s 779us/step - loss: 0.6837 - accuracy: 0.7491\n",
      "Epoch 136/200\n",
      "248/248 [==============================] - 0s 847us/step - loss: 0.6826 - accuracy: 0.7491\n",
      "Epoch 137/200\n",
      "248/248 [==============================] - 0s 782us/step - loss: 0.6834 - accuracy: 0.7470\n",
      "Epoch 138/200\n",
      "248/248 [==============================] - 0s 773us/step - loss: 0.6820 - accuracy: 0.7494\n",
      "Epoch 139/200\n",
      "248/248 [==============================] - 0s 774us/step - loss: 0.6811 - accuracy: 0.7518\n",
      "Epoch 140/200\n",
      "248/248 [==============================] - 0s 766us/step - loss: 0.6817 - accuracy: 0.7508\n",
      "Epoch 141/200\n",
      "248/248 [==============================] - 0s 769us/step - loss: 0.6826 - accuracy: 0.7489\n",
      "Epoch 142/200\n",
      "248/248 [==============================] - 0s 775us/step - loss: 0.6807 - accuracy: 0.7498\n",
      "Epoch 143/200\n",
      "248/248 [==============================] - 0s 784us/step - loss: 0.6807 - accuracy: 0.7502\n",
      "Epoch 144/200\n",
      "248/248 [==============================] - 0s 772us/step - loss: 0.6804 - accuracy: 0.7492\n",
      "Epoch 145/200\n",
      "248/248 [==============================] - 0s 772us/step - loss: 0.6801 - accuracy: 0.7486\n",
      "Epoch 146/200\n",
      "248/248 [==============================] - 0s 778us/step - loss: 0.6810 - accuracy: 0.7501\n",
      "Epoch 147/200\n",
      "248/248 [==============================] - 0s 766us/step - loss: 0.6788 - accuracy: 0.7507\n",
      "Epoch 148/200\n",
      "248/248 [==============================] - 0s 784us/step - loss: 0.6784 - accuracy: 0.7521\n",
      "Epoch 149/200\n",
      "248/248 [==============================] - 0s 765us/step - loss: 0.6772 - accuracy: 0.7518\n",
      "Epoch 150/200\n",
      "248/248 [==============================] - 0s 806us/step - loss: 0.6773 - accuracy: 0.7521\n",
      "Epoch 151/200\n",
      "248/248 [==============================] - 0s 785us/step - loss: 0.6774 - accuracy: 0.7504\n",
      "Epoch 152/200\n",
      "248/248 [==============================] - 0s 851us/step - loss: 0.6764 - accuracy: 0.7524\n",
      "Epoch 153/200\n",
      "248/248 [==============================] - 0s 831us/step - loss: 0.6765 - accuracy: 0.7506\n",
      "Epoch 154/200\n",
      "248/248 [==============================] - 0s 787us/step - loss: 0.6749 - accuracy: 0.7521\n",
      "Epoch 155/200\n",
      "248/248 [==============================] - 0s 821us/step - loss: 0.6757 - accuracy: 0.7511\n",
      "Epoch 156/200\n",
      "248/248 [==============================] - 0s 838us/step - loss: 0.6753 - accuracy: 0.7521\n",
      "Epoch 157/200\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.6742 - accuracy: 0.7511\n",
      "Epoch 158/200\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.6737 - accuracy: 0.7534\n",
      "Epoch 159/200\n",
      "248/248 [==============================] - 0s 811us/step - loss: 0.6735 - accuracy: 0.7532\n",
      "Epoch 160/200\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.6733 - accuracy: 0.7539\n",
      "Epoch 161/200\n",
      "248/248 [==============================] - 0s 905us/step - loss: 0.6733 - accuracy: 0.7506\n",
      "Epoch 162/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.6720 - accuracy: 0.7532\n",
      "Epoch 163/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.6723 - accuracy: 0.7538\n",
      "Epoch 164/200\n",
      "248/248 [==============================] - 0s 815us/step - loss: 0.6722 - accuracy: 0.7548\n",
      "Epoch 165/200\n",
      "248/248 [==============================] - 0s 815us/step - loss: 0.6712 - accuracy: 0.7543\n",
      "Epoch 166/200\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.6700 - accuracy: 0.7518\n",
      "Epoch 167/200\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.6707 - accuracy: 0.7523\n",
      "Epoch 168/200\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.6694 - accuracy: 0.7528\n",
      "Epoch 169/200\n",
      "248/248 [==============================] - 0s 876us/step - loss: 0.6696 - accuracy: 0.7541\n",
      "Epoch 170/200\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.6697 - accuracy: 0.7537\n",
      "Epoch 171/200\n",
      "248/248 [==============================] - 0s 819us/step - loss: 0.6679 - accuracy: 0.7542\n",
      "Epoch 172/200\n",
      "248/248 [==============================] - 0s 815us/step - loss: 0.6673 - accuracy: 0.7552\n",
      "Epoch 173/200\n",
      "248/248 [==============================] - 0s 811us/step - loss: 0.6674 - accuracy: 0.7555\n",
      "Epoch 174/200\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.6678 - accuracy: 0.7542\n",
      "Epoch 175/200\n",
      "248/248 [==============================] - 0s 820us/step - loss: 0.6666 - accuracy: 0.7561\n",
      "Epoch 176/200\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.6667 - accuracy: 0.7554\n",
      "Epoch 177/200\n",
      "248/248 [==============================] - 0s 819us/step - loss: 0.6671 - accuracy: 0.7566\n",
      "Epoch 178/200\n",
      "248/248 [==============================] - 0s 828us/step - loss: 0.6673 - accuracy: 0.7549\n",
      "Epoch 179/200\n",
      "248/248 [==============================] - 0s 824us/step - loss: 0.6660 - accuracy: 0.7558\n",
      "Epoch 180/200\n",
      "248/248 [==============================] - 0s 827us/step - loss: 0.6668 - accuracy: 0.7558\n",
      "Epoch 181/200\n",
      "248/248 [==============================] - 0s 965us/step - loss: 0.6642 - accuracy: 0.7557\n",
      "Epoch 182/200\n",
      "248/248 [==============================] - 0s 823us/step - loss: 0.6648 - accuracy: 0.7581\n",
      "Epoch 183/200\n",
      "248/248 [==============================] - 0s 877us/step - loss: 0.6643 - accuracy: 0.7559\n",
      "Epoch 184/200\n",
      "248/248 [==============================] - 0s 815us/step - loss: 0.6636 - accuracy: 0.7564\n",
      "Epoch 185/200\n",
      "248/248 [==============================] - 0s 887us/step - loss: 0.6636 - accuracy: 0.7564\n",
      "Epoch 186/200\n",
      "248/248 [==============================] - 0s 816us/step - loss: 0.6625 - accuracy: 0.7564\n",
      "Epoch 187/200\n",
      "248/248 [==============================] - 0s 810us/step - loss: 0.6628 - accuracy: 0.7565\n",
      "Epoch 188/200\n",
      "248/248 [==============================] - 0s 812us/step - loss: 0.6625 - accuracy: 0.7560\n",
      "Epoch 189/200\n",
      "248/248 [==============================] - 0s 801us/step - loss: 0.6618 - accuracy: 0.7557\n",
      "Epoch 190/200\n",
      "248/248 [==============================] - 0s 801us/step - loss: 0.6617 - accuracy: 0.7562\n",
      "Epoch 191/200\n",
      "248/248 [==============================] - 0s 800us/step - loss: 0.6614 - accuracy: 0.7563\n",
      "Epoch 192/200\n",
      "248/248 [==============================] - 0s 805us/step - loss: 0.6598 - accuracy: 0.7585\n",
      "Epoch 193/200\n",
      "248/248 [==============================] - 0s 804us/step - loss: 0.6620 - accuracy: 0.7574\n",
      "Epoch 194/200\n",
      "248/248 [==============================] - 0s 808us/step - loss: 0.6623 - accuracy: 0.7581\n",
      "Epoch 195/200\n",
      "248/248 [==============================] - 0s 819us/step - loss: 0.6606 - accuracy: 0.7576\n",
      "Epoch 196/200\n",
      "248/248 [==============================] - 0s 803us/step - loss: 0.6594 - accuracy: 0.7576\n",
      "Epoch 197/200\n",
      "248/248 [==============================] - 0s 798us/step - loss: 0.6579 - accuracy: 0.7582\n",
      "Epoch 198/200\n",
      "248/248 [==============================] - 0s 799us/step - loss: 0.6595 - accuracy: 0.7578\n",
      "Epoch 199/200\n",
      "248/248 [==============================] - 0s 800us/step - loss: 0.6603 - accuracy: 0.7590\n",
      "Epoch 200/200\n",
      "248/248 [==============================] - 0s 832us/step - loss: 0.6581 - accuracy: 0.7602\n",
      "124/124 [==============================] - 0s 752us/step\n",
      "(3956, 6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.28      0.36       277\n",
      "           1       0.84      0.81      0.82      1641\n",
      "           2       0.62      0.26      0.37       107\n",
      "           3       0.59      0.44      0.50        94\n",
      "           4       0.49      0.73      0.59       763\n",
      "           5       0.78      0.70      0.74      1074\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      3956\n",
      "   macro avg       0.64      0.54      0.56      3956\n",
      "weighted avg       0.72      0.71      0.70      3956\n",
      " samples avg       0.71      0.71      0.71      3956\n",
      "\n",
      "Accuracy: 0.705005055611729\n"
     ]
    }
   ],
   "source": [
    "ANN_classificate(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def CNN_classificate(train_text, test_text, train_label, test_label):\n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,ngram_range=(1,1),lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text).toarray()\n",
    "    test_dtm=tfidf.transform(test_text).toarray()\n",
    "    lb = LabelBinarizer()            \n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 6000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 8000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    #train_dtm,train_label = sm.fit_resample(train_dtm, train_label)\n",
    "    train_y=lb.fit_transform(train_label)\n",
    "    test_y=lb.transform(test_label)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(train_dtm.shape[1],1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled=scaler.fit_transform(train_dtm)\n",
    "    X_test_scaled=scaler.fit_transform(test_dtm)\n",
    "\n",
    "    model.fit(X_train_scaled, train_y, epochs=100, batch_size=64)\n",
    "\n",
    "    y_pred=model.predict(X_test_scaled)\n",
    "\n",
    "    y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_pred_binary = np.zeros_like(y_pred)\n",
    "    for i, class_index in enumerate(y_pred_argmax):\n",
    "        y_pred_binary[i, class_index] = 1\n",
    "\n",
    "\n",
    "\n",
    "    print(y_pred_binary.shape)\n",
    "    report=classification_report(test_y,y_pred_binary)\n",
    "    print(report)\n",
    "    print(\"Accuracy:\", accuracy_score(test_y, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 1.1291 - accuracy: 0.5827\n",
      "Epoch 2/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.9386 - accuracy: 0.6649\n",
      "Epoch 3/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.9043 - accuracy: 0.6793\n",
      "Epoch 4/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8804 - accuracy: 0.6826\n",
      "Epoch 5/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.8663 - accuracy: 0.6906\n",
      "Epoch 6/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8527 - accuracy: 0.6964\n",
      "Epoch 7/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8490 - accuracy: 0.6971\n",
      "Epoch 8/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8393 - accuracy: 0.6996\n",
      "Epoch 9/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8386 - accuracy: 0.6963\n",
      "Epoch 10/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8317 - accuracy: 0.6994\n",
      "Epoch 11/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8321 - accuracy: 0.7016\n",
      "Epoch 12/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8330 - accuracy: 0.7030\n",
      "Epoch 13/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8251 - accuracy: 0.7055\n",
      "Epoch 14/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.8218 - accuracy: 0.7072\n",
      "Epoch 15/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8151 - accuracy: 0.7058\n",
      "Epoch 16/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8176 - accuracy: 0.7045\n",
      "Epoch 17/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8058 - accuracy: 0.7065\n",
      "Epoch 18/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8112 - accuracy: 0.7063\n",
      "Epoch 19/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8097 - accuracy: 0.7061\n",
      "Epoch 20/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8052 - accuracy: 0.7093\n",
      "Epoch 21/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8070 - accuracy: 0.7114\n",
      "Epoch 22/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.8018 - accuracy: 0.7104\n",
      "Epoch 23/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7989 - accuracy: 0.7090\n",
      "Epoch 24/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7977 - accuracy: 0.7126\n",
      "Epoch 25/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.8040 - accuracy: 0.7118\n",
      "Epoch 26/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7941 - accuracy: 0.7111\n",
      "Epoch 27/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7986 - accuracy: 0.7126\n",
      "Epoch 28/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7911 - accuracy: 0.7125\n",
      "Epoch 29/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7942 - accuracy: 0.7082\n",
      "Epoch 30/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7846 - accuracy: 0.7142\n",
      "Epoch 31/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7884 - accuracy: 0.7114\n",
      "Epoch 32/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7876 - accuracy: 0.7102\n",
      "Epoch 33/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7827 - accuracy: 0.7135\n",
      "Epoch 34/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7863 - accuracy: 0.7149\n",
      "Epoch 35/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7825 - accuracy: 0.7130\n",
      "Epoch 36/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7813 - accuracy: 0.7142\n",
      "Epoch 37/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7792 - accuracy: 0.7167\n",
      "Epoch 38/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7805 - accuracy: 0.7131\n",
      "Epoch 39/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7832 - accuracy: 0.7142\n",
      "Epoch 40/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7814 - accuracy: 0.7139\n",
      "Epoch 41/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7756 - accuracy: 0.7180\n",
      "Epoch 42/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7746 - accuracy: 0.7134\n",
      "Epoch 43/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7751 - accuracy: 0.7139\n",
      "Epoch 44/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7764 - accuracy: 0.7155\n",
      "Epoch 45/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7747 - accuracy: 0.7166\n",
      "Epoch 46/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7755 - accuracy: 0.7161\n",
      "Epoch 47/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7698 - accuracy: 0.7178\n",
      "Epoch 48/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7700 - accuracy: 0.7199\n",
      "Epoch 49/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7764 - accuracy: 0.7144\n",
      "Epoch 50/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7654 - accuracy: 0.7177\n",
      "Epoch 51/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7752 - accuracy: 0.7153\n",
      "Epoch 52/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7680 - accuracy: 0.7198\n",
      "Epoch 53/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7670 - accuracy: 0.7169\n",
      "Epoch 54/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7646 - accuracy: 0.7177\n",
      "Epoch 55/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7634 - accuracy: 0.7176\n",
      "Epoch 56/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7653 - accuracy: 0.7166\n",
      "Epoch 57/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7684 - accuracy: 0.7181\n",
      "Epoch 58/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7606 - accuracy: 0.7208\n",
      "Epoch 59/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7620 - accuracy: 0.7176\n",
      "Epoch 60/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7552 - accuracy: 0.7197\n",
      "Epoch 61/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7655 - accuracy: 0.7191\n",
      "Epoch 62/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7571 - accuracy: 0.7190\n",
      "Epoch 63/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7621 - accuracy: 0.7169\n",
      "Epoch 64/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7592 - accuracy: 0.7215\n",
      "Epoch 65/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7592 - accuracy: 0.7197\n",
      "Epoch 66/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7564 - accuracy: 0.7217\n",
      "Epoch 67/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7596 - accuracy: 0.7161\n",
      "Epoch 68/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7567 - accuracy: 0.7204\n",
      "Epoch 69/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7623 - accuracy: 0.7206\n",
      "Epoch 70/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7506 - accuracy: 0.7224\n",
      "Epoch 71/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7528 - accuracy: 0.7230\n",
      "Epoch 72/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7514 - accuracy: 0.7164\n",
      "Epoch 73/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7531 - accuracy: 0.7203\n",
      "Epoch 74/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7532 - accuracy: 0.7195\n",
      "Epoch 75/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7539 - accuracy: 0.7216\n",
      "Epoch 76/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7489 - accuracy: 0.7224\n",
      "Epoch 77/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7520 - accuracy: 0.7204\n",
      "Epoch 78/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7551 - accuracy: 0.7198\n",
      "Epoch 79/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7484 - accuracy: 0.7234\n",
      "Epoch 80/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7449 - accuracy: 0.7221\n",
      "Epoch 81/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7457 - accuracy: 0.7231\n",
      "Epoch 82/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7466 - accuracy: 0.7210\n",
      "Epoch 83/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7431 - accuracy: 0.7216\n",
      "Epoch 84/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7430 - accuracy: 0.7228\n",
      "Epoch 85/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7432 - accuracy: 0.7253\n",
      "Epoch 86/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7410 - accuracy: 0.7235\n",
      "Epoch 87/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7400 - accuracy: 0.7222\n",
      "Epoch 88/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7422 - accuracy: 0.7214\n",
      "Epoch 89/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7441 - accuracy: 0.7237\n",
      "Epoch 90/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7418 - accuracy: 0.7257\n",
      "Epoch 91/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7439 - accuracy: 0.7198\n",
      "Epoch 92/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7372 - accuracy: 0.7272\n",
      "Epoch 93/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7389 - accuracy: 0.7221\n",
      "Epoch 94/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7353 - accuracy: 0.7288\n",
      "Epoch 95/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7393 - accuracy: 0.7241\n",
      "Epoch 96/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7378 - accuracy: 0.7271\n",
      "Epoch 97/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7380 - accuracy: 0.7228\n",
      "Epoch 98/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7352 - accuracy: 0.7230\n",
      "Epoch 99/100\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.7375 - accuracy: 0.7260\n",
      "Epoch 100/100\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.7381 - accuracy: 0.7255\n",
      "124/124 [==============================] - 0s 1ms/step\n",
      "(3956, 6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.29      0.38       277\n",
      "           1       0.82      0.85      0.83      1641\n",
      "           2       0.78      0.27      0.40       107\n",
      "           3       0.60      0.40      0.48        94\n",
      "           4       0.52      0.67      0.58       763\n",
      "           5       0.76      0.73      0.74      1074\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      3956\n",
      "   macro avg       0.67      0.53      0.57      3956\n",
      "weighted avg       0.72      0.71      0.71      3956\n",
      " samples avg       0.71      0.71      0.71      3956\n",
      "\n",
      "Accuracy: 0.7143579373104145\n"
     ]
    }
   ],
   "source": [
    "CNN_classificate(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 1.1291 - accuracy: 0.5827\n",
    "Epoch 2/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.9386 - accuracy: 0.6649\n",
    "Epoch 3/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.9043 - accuracy: 0.6793\n",
    "Epoch 4/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8804 - accuracy: 0.6826\n",
    "Epoch 5/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.8663 - accuracy: 0.6906\n",
    "Epoch 6/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8527 - accuracy: 0.6964\n",
    "Epoch 7/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8490 - accuracy: 0.6971\n",
    "Epoch 8/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8393 - accuracy: 0.6996\n",
    "Epoch 9/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8386 - accuracy: 0.6963\n",
    "Epoch 10/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8317 - accuracy: 0.6994\n",
    "Epoch 11/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8321 - accuracy: 0.7016\n",
    "Epoch 12/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8330 - accuracy: 0.7030\n",
    "Epoch 13/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8251 - accuracy: 0.7055\n",
    "Epoch 14/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.8218 - accuracy: 0.7072\n",
    "Epoch 15/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8151 - accuracy: 0.7058\n",
    "Epoch 16/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8176 - accuracy: 0.7045\n",
    "Epoch 17/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8058 - accuracy: 0.7065\n",
    "Epoch 18/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8112 - accuracy: 0.7063\n",
    "Epoch 19/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8097 - accuracy: 0.7061\n",
    "Epoch 20/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8052 - accuracy: 0.7093\n",
    "Epoch 21/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8070 - accuracy: 0.7114\n",
    "Epoch 22/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.8018 - accuracy: 0.7104\n",
    "Epoch 23/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7989 - accuracy: 0.7090\n",
    "Epoch 24/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7977 - accuracy: 0.7126\n",
    "Epoch 25/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.8040 - accuracy: 0.7118\n",
    "Epoch 26/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7941 - accuracy: 0.7111\n",
    "Epoch 27/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7986 - accuracy: 0.7126\n",
    "Epoch 28/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7911 - accuracy: 0.7125\n",
    "Epoch 29/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7942 - accuracy: 0.7082\n",
    "Epoch 30/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7846 - accuracy: 0.7142\n",
    "Epoch 31/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7884 - accuracy: 0.7114\n",
    "Epoch 32/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7876 - accuracy: 0.7102\n",
    "Epoch 33/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7827 - accuracy: 0.7135\n",
    "Epoch 34/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7863 - accuracy: 0.7149\n",
    "Epoch 35/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7825 - accuracy: 0.7130\n",
    "Epoch 36/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7813 - accuracy: 0.7142\n",
    "Epoch 37/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7792 - accuracy: 0.7167\n",
    "Epoch 38/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7805 - accuracy: 0.7131\n",
    "Epoch 39/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7832 - accuracy: 0.7142\n",
    "Epoch 40/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7814 - accuracy: 0.7139\n",
    "Epoch 41/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7756 - accuracy: 0.7180\n",
    "Epoch 42/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7746 - accuracy: 0.7134\n",
    "Epoch 43/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7751 - accuracy: 0.7139\n",
    "Epoch 44/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7764 - accuracy: 0.7155\n",
    "Epoch 45/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7747 - accuracy: 0.7166\n",
    "Epoch 46/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7755 - accuracy: 0.7161\n",
    "Epoch 47/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7698 - accuracy: 0.7178\n",
    "Epoch 48/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7700 - accuracy: 0.7199\n",
    "Epoch 49/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7764 - accuracy: 0.7144\n",
    "Epoch 50/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7654 - accuracy: 0.7177\n",
    "Epoch 51/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7752 - accuracy: 0.7153\n",
    "Epoch 52/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7680 - accuracy: 0.7198\n",
    "Epoch 53/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7670 - accuracy: 0.7169\n",
    "Epoch 54/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7646 - accuracy: 0.7177\n",
    "Epoch 55/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7634 - accuracy: 0.7176\n",
    "Epoch 56/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7653 - accuracy: 0.7166\n",
    "Epoch 57/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7684 - accuracy: 0.7181\n",
    "Epoch 58/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7606 - accuracy: 0.7208\n",
    "Epoch 59/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7620 - accuracy: 0.7176\n",
    "Epoch 60/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7552 - accuracy: 0.7197\n",
    "Epoch 61/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7655 - accuracy: 0.7191\n",
    "Epoch 62/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7571 - accuracy: 0.7190\n",
    "Epoch 63/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7621 - accuracy: 0.7169\n",
    "Epoch 64/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7592 - accuracy: 0.7215\n",
    "Epoch 65/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7592 - accuracy: 0.7197\n",
    "Epoch 66/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7564 - accuracy: 0.7217\n",
    "Epoch 67/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7596 - accuracy: 0.7161\n",
    "Epoch 68/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7567 - accuracy: 0.7204\n",
    "Epoch 69/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7623 - accuracy: 0.7206\n",
    "Epoch 70/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7506 - accuracy: 0.7224\n",
    "Epoch 71/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7528 - accuracy: 0.7230\n",
    "Epoch 72/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7514 - accuracy: 0.7164\n",
    "Epoch 73/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7531 - accuracy: 0.7203\n",
    "Epoch 74/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7532 - accuracy: 0.7195\n",
    "Epoch 75/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7539 - accuracy: 0.7216\n",
    "Epoch 76/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7489 - accuracy: 0.7224\n",
    "Epoch 77/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7520 - accuracy: 0.7204\n",
    "Epoch 78/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7551 - accuracy: 0.7198\n",
    "Epoch 79/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7484 - accuracy: 0.7234\n",
    "Epoch 80/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7449 - accuracy: 0.7221\n",
    "Epoch 81/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7457 - accuracy: 0.7231\n",
    "Epoch 82/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7466 - accuracy: 0.7210\n",
    "Epoch 83/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7431 - accuracy: 0.7216\n",
    "Epoch 84/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7430 - accuracy: 0.7228\n",
    "Epoch 85/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7432 - accuracy: 0.7253\n",
    "Epoch 86/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7410 - accuracy: 0.7235\n",
    "Epoch 87/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7400 - accuracy: 0.7222\n",
    "Epoch 88/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7422 - accuracy: 0.7214\n",
    "Epoch 89/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7441 - accuracy: 0.7237\n",
    "Epoch 90/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7418 - accuracy: 0.7257\n",
    "Epoch 91/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7439 - accuracy: 0.7198\n",
    "Epoch 92/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7372 - accuracy: 0.7272\n",
    "Epoch 93/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7389 - accuracy: 0.7221\n",
    "Epoch 94/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7353 - accuracy: 0.7288\n",
    "Epoch 95/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7393 - accuracy: 0.7241\n",
    "Epoch 96/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7378 - accuracy: 0.7271\n",
    "Epoch 97/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7380 - accuracy: 0.7228\n",
    "Epoch 98/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7352 - accuracy: 0.7230\n",
    "Epoch 99/100\n",
    "248/248 [==============================] - 1s 3ms/step - loss: 0.7375 - accuracy: 0.7260\n",
    "Epoch 100/100\n",
    "248/248 [==============================] - 1s 4ms/step - loss: 0.7381 - accuracy: 0.7255\n",
    "124/124 [==============================] - 0s 1ms/step\n",
    "(3956, 6)\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.55      0.29      0.38       277\n",
    "           1       0.82      0.85      0.83      1641\n",
    "           2       0.78      0.27      0.40       107\n",
    "           3       0.60      0.40      0.48        94\n",
    "           4       0.52      0.67      0.58       763\n",
    "           5       0.76      0.73      0.74      1074\n",
    "\n",
    "   micro avg       0.71      0.71      0.71      3956\n",
    "   macro avg       0.67      0.53      0.57      3956\n",
    "weighted avg       0.72      0.71      0.71      3956\n",
    " samples avg       0.71      0.71      0.71      3956\n",
    "\n",
    "Accuracy: 0.7143579373104145\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "def CNN_classificate(train_text, test_text, train_label, test_label):\n",
    "    tfidf=TfidfVectorizer(stop_words=\"english\", min_df=0.005,ngram_range=(1,1),lowercase=True)\n",
    "    train_dtm=tfidf.fit_transform(train_text).toarray()\n",
    "    test_dtm=tfidf.transform(test_text).toarray()\n",
    "    lb = LabelBinarizer()            \n",
    "    sampling_strategy = {'food': 10000, 'atmosphere': 6000, 'location': 4000, 'service': 8000, 'none': 8000, 'hygiene': 8000}\n",
    "    sm = SMOTE(random_state=42,sampling_strategy=sampling_strategy)\n",
    "    train_dtm,train_label = sm.fit_resample(train_dtm, train_label)\n",
    "    train_y=lb.fit_transform(train_label)\n",
    "    test_y=lb.transform(test_label)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(train_dtm.shape[1],1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled=scaler.fit_transform(train_dtm)\n",
    "    X_test_scaled=scaler.fit_transform(test_dtm)\n",
    "\n",
    "    model.fit(X_train_scaled, train_y, epochs=100, batch_size=64)\n",
    "\n",
    "    y_pred=model.predict(X_test_scaled)\n",
    "\n",
    "    y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_pred_binary = np.zeros_like(y_pred)\n",
    "    for i, class_index in enumerate(y_pred_argmax):\n",
    "        y_pred_binary[i, class_index] = 1\n",
    "\n",
    "\n",
    "\n",
    "    print(y_pred_binary.shape)\n",
    "    report=classification_report(test_y,y_pred_binary)\n",
    "    print(report)\n",
    "    print(\"Accuracy:\", accuracy_score(test_y, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "688/688 [==============================] - 3s 3ms/step - loss: 1.1297 - accuracy: 0.5795\n",
      "Epoch 2/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.9827 - accuracy: 0.6293\n",
      "Epoch 3/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.9479 - accuracy: 0.6438\n",
      "Epoch 4/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.9276 - accuracy: 0.6510\n",
      "Epoch 5/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.9126 - accuracy: 0.6525\n",
      "Epoch 6/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.9023 - accuracy: 0.6558\n",
      "Epoch 7/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8913 - accuracy: 0.6595\n",
      "Epoch 8/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8832 - accuracy: 0.6649\n",
      "Epoch 9/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8753 - accuracy: 0.6665\n",
      "Epoch 10/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8696 - accuracy: 0.6653\n",
      "Epoch 11/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8644 - accuracy: 0.6670\n",
      "Epoch 12/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8574 - accuracy: 0.6709\n",
      "Epoch 13/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8500 - accuracy: 0.6737\n",
      "Epoch 14/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8462 - accuracy: 0.6763\n",
      "Epoch 15/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8443 - accuracy: 0.6764\n",
      "Epoch 16/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8407 - accuracy: 0.6768\n",
      "Epoch 17/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8392 - accuracy: 0.6783\n",
      "Epoch 18/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8347 - accuracy: 0.6817\n",
      "Epoch 19/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8318 - accuracy: 0.6782\n",
      "Epoch 20/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8274 - accuracy: 0.6813\n",
      "Epoch 21/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8265 - accuracy: 0.6832\n",
      "Epoch 22/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8219 - accuracy: 0.6817\n",
      "Epoch 23/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8183 - accuracy: 0.6851\n",
      "Epoch 24/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8174 - accuracy: 0.6832\n",
      "Epoch 25/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8163 - accuracy: 0.6862\n",
      "Epoch 26/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8128 - accuracy: 0.6879\n",
      "Epoch 27/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8115 - accuracy: 0.6865\n",
      "Epoch 28/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8079 - accuracy: 0.6870\n",
      "Epoch 29/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8036 - accuracy: 0.6875\n",
      "Epoch 30/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8042 - accuracy: 0.6905\n",
      "Epoch 31/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8017 - accuracy: 0.6900\n",
      "Epoch 32/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.8002 - accuracy: 0.6885\n",
      "Epoch 33/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7977 - accuracy: 0.6923\n",
      "Epoch 34/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7975 - accuracy: 0.6904\n",
      "Epoch 35/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7920 - accuracy: 0.6936\n",
      "Epoch 36/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7932 - accuracy: 0.6945\n",
      "Epoch 37/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7905 - accuracy: 0.6942\n",
      "Epoch 38/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7856 - accuracy: 0.6951\n",
      "Epoch 39/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7911 - accuracy: 0.6962\n",
      "Epoch 40/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7893 - accuracy: 0.6946\n",
      "Epoch 41/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7887 - accuracy: 0.6942\n",
      "Epoch 42/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7890 - accuracy: 0.6931\n",
      "Epoch 43/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7833 - accuracy: 0.6980\n",
      "Epoch 44/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7829 - accuracy: 0.6988\n",
      "Epoch 45/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7848 - accuracy: 0.6945\n",
      "Epoch 46/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7774 - accuracy: 0.6980\n",
      "Epoch 47/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7778 - accuracy: 0.6960\n",
      "Epoch 48/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7738 - accuracy: 0.7004\n",
      "Epoch 49/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7717 - accuracy: 0.7013\n",
      "Epoch 50/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7748 - accuracy: 0.6998\n",
      "Epoch 51/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7676 - accuracy: 0.7007\n",
      "Epoch 52/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7714 - accuracy: 0.7008\n",
      "Epoch 53/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7674 - accuracy: 0.7038\n",
      "Epoch 54/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7702 - accuracy: 0.7005\n",
      "Epoch 55/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7677 - accuracy: 0.7024\n",
      "Epoch 56/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7659 - accuracy: 0.7024\n",
      "Epoch 57/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7692 - accuracy: 0.7052\n",
      "Epoch 58/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7644 - accuracy: 0.7052\n",
      "Epoch 59/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7617 - accuracy: 0.7045\n",
      "Epoch 60/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7625 - accuracy: 0.7038\n",
      "Epoch 61/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7641 - accuracy: 0.7031\n",
      "Epoch 62/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7622 - accuracy: 0.7054\n",
      "Epoch 63/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7611 - accuracy: 0.7028\n",
      "Epoch 64/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7621 - accuracy: 0.7050\n",
      "Epoch 65/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7588 - accuracy: 0.7066\n",
      "Epoch 66/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7591 - accuracy: 0.7048\n",
      "Epoch 67/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7553 - accuracy: 0.7071\n",
      "Epoch 68/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7600 - accuracy: 0.7069\n",
      "Epoch 69/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7590 - accuracy: 0.7057\n",
      "Epoch 70/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7560 - accuracy: 0.7064\n",
      "Epoch 71/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7556 - accuracy: 0.7065\n",
      "Epoch 72/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7508 - accuracy: 0.7092\n",
      "Epoch 73/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7535 - accuracy: 0.7080\n",
      "Epoch 74/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7571 - accuracy: 0.7059\n",
      "Epoch 75/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7488 - accuracy: 0.7067\n",
      "Epoch 76/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7497 - accuracy: 0.7092\n",
      "Epoch 77/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7530 - accuracy: 0.7066\n",
      "Epoch 78/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7521 - accuracy: 0.7065\n",
      "Epoch 79/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7491 - accuracy: 0.7080\n",
      "Epoch 80/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7520 - accuracy: 0.7085\n",
      "Epoch 81/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7520 - accuracy: 0.7085\n",
      "Epoch 82/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7457 - accuracy: 0.7087\n",
      "Epoch 83/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7480 - accuracy: 0.7100\n",
      "Epoch 84/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7480 - accuracy: 0.7086\n",
      "Epoch 85/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7477 - accuracy: 0.7118\n",
      "Epoch 86/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7519 - accuracy: 0.7086\n",
      "Epoch 87/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7459 - accuracy: 0.7093\n",
      "Epoch 88/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7446 - accuracy: 0.7125\n",
      "Epoch 89/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7472 - accuracy: 0.7088\n",
      "Epoch 90/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7395 - accuracy: 0.7121\n",
      "Epoch 91/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7440 - accuracy: 0.7093\n",
      "Epoch 92/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7434 - accuracy: 0.7105\n",
      "Epoch 93/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7462 - accuracy: 0.7114\n",
      "Epoch 94/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7438 - accuracy: 0.7099\n",
      "Epoch 95/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7450 - accuracy: 0.7111\n",
      "Epoch 96/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7410 - accuracy: 0.7133\n",
      "Epoch 97/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7400 - accuracy: 0.7119\n",
      "Epoch 98/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7410 - accuracy: 0.7117\n",
      "Epoch 99/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7412 - accuracy: 0.7103\n",
      "Epoch 100/100\n",
      "688/688 [==============================] - 2s 3ms/step - loss: 0.7367 - accuracy: 0.7144\n",
      "124/124 [==============================] - 0s 1ms/step\n",
      "(3956, 6)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.38      0.40       277\n",
      "           1       0.85      0.76      0.80      1641\n",
      "           2       0.11      0.55      0.18       107\n",
      "           3       0.49      0.47      0.48        94\n",
      "           4       0.54      0.46      0.50       763\n",
      "           5       0.80      0.70      0.74      1074\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      3956\n",
      "   macro avg       0.53      0.55      0.52      3956\n",
      "weighted avg       0.72      0.65      0.68      3956\n",
      " samples avg       0.65      0.65      0.65      3956\n",
      "\n",
      "Accuracy: 0.64737108190091\n"
     ]
    }
   ],
   "source": [
    "CNN_classificate(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
